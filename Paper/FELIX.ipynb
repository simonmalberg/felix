{"cells":[{"cell_type":"markdown","metadata":{"id":"ZC2TEsX5sPQj"},"source":["# Prepare Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1HBYYYFr7Rc"},"outputs":[],"source":["!pip install transformers[torch] datasets pydantic==1.10 langchain[llms] openai tiktoken hdbscan wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0DJiQH13cPD"},"outputs":[],"source":["import os\n","import wandb\n","\n","# Set API keys as environment variables\n","os.environ[\"OPENAI_API_KEY\"] = \"...\"\n","os.environ[\"OPENAI_ORG\"] = \"...\"\n","os.environ[\"WANDB_KEY\"] = \"...\"\n","\n","# Login to Weights & Biases for experiment tracking\n","wandb.login(key=os.environ[\"WANDB_KEY\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-_LKY9v8Wrv"},"outputs":[],"source":["seed = 42"]},{"cell_type":"markdown","metadata":{"id":"p_splIj31pfw"},"source":["# Prepare Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NI6M7TIzI3HS"},"outputs":[],"source":["# Import HuggingFace datasets package\n","import datasets\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Define container class for datasets\n","class Dataset:\n","\n","    def __init__(self, id, short_name, X_train, X_test, y_train, y_test, pos_class, context_train, context_test, train_zero_shot=False):\n","        self.id = id\n","        self.short_name = short_name\n","        self.X_train = X_train\n","        self.X_test = X_test\n","        self.y_train = y_train\n","        self.y_test = y_test\n","        self.pos_class = pos_class # Class that counts as positive (for measuring precision, recall, and F1 score)\n","        self.context_train = context_train\n","        self.context_test = context_test\n","        self.train_zero_shot = train_zero_shot\n","\n","\n","    def __str__(self):\n","        return \"Dataset(\\n\" + f\"\\tid='{self.id}'\\n\" + f\"\\tshort_name='{self.short_name}'\\n\" + f\"\\tcontext_train='{self.context_train}'\\n\" + f\"\\tcontext_test='{self.context_test}'\\n\" + f\"\\tclasses={list(pd.concat([self.y_train, self.y_test]).unique())}\\n\" + f\"\\tpos_class='{self.pos_class}'\\n\" + f\"\\ttrain_zero_shot={self.train_zero_shot}\\n\" + f\"\\tX_train.shape={self.X_train.shape}\\n\" + f\"\\tX_test.shape={self.X_test.shape}\\n\" + f\"\\ty_train.shape={self.y_train.shape}\\n\" + f\"\\ty_test.shape={self.y_test.shape}\\n\" + \")\"\n","\n","\n","    def __repr__(self):\n","        return self.__str__()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrhn5N1xKIje"},"outputs":[],"source":["# Initialize list of datasets\n","dataset_list = {}\n","\n","# Set overall sample size\n","n_train = 100\n","n_test = 100"]},{"cell_type":"markdown","metadata":{"id":"BEcYM8nv_80n"},"source":["## Fake Scientific Papers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEtiVn248QLH"},"outputs":[],"source":["# Load the original training dataset\n","train = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"train+gpt3\", split=\"train\")\n","\n","# Randomly shuffle the dataset\n","train_shuffled = train.shuffle(seed=seed)\n","\n","# Separate the different sources/generators\n","train_real = train_shuffled.filter(lambda x: x[\"src\"] == \"real\")\n","train_scigen = train_shuffled.filter(lambda x: x[\"src\"] == \"scigen\")\n","train_galactica = train_shuffled.filter(lambda x: x[\"src\"] == \"galactica\")\n","train_gpt2 = train_shuffled.filter(lambda x: x[\"src\"] == \"gpt2\")\n","train_gpt3 = train_shuffled.filter(lambda x: x[\"src\"] == \"gpt3\")\n","train_chatgpt = train_shuffled.filter(lambda x: x[\"src\"] == \"chatgpt\")\n","\n","# Create one training dataset with alternating real and fake examples and equal number of examples from each fake source\n","train_fake = datasets.interleave_datasets([train_scigen, train_galactica, train_gpt2, train_gpt3, train_chatgpt])\n","data_train = datasets.interleave_datasets([train_real, train_fake])\n","\n","\n","\n","# Load the original test datasets\n","test = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"classifier_input\", split=\"test\")\n","ood_gpt3 = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"ood_gpt3\", split=\"test\")\n","\n","# Randomly shuffle the datasets\n","test_shuffled = test.shuffle(seed=seed)\n","test_gpt3 = ood_gpt3.shuffle(seed=seed)\n","\n","# Separate the different sources/generators\n","test_real = test_shuffled.filter(lambda x: x[\"src\"] == \"real\")\n","test_scigen = test_shuffled.filter(lambda x: x[\"src\"] == \"scigen\")\n","test_galactica = test_shuffled.filter(lambda x: x[\"src\"] == \"galactica\")\n","test_gpt2 = test_shuffled.filter(lambda x: x[\"src\"] == \"gpt2\")\n","test_chatgpt = test_shuffled.filter(lambda x: x[\"src\"] == \"chatgpt\")\n","\n","# Create one test dataset with alternating real and fake examples and equal number of examples from each fake source\n","test_fake = datasets.interleave_datasets([test_scigen, test_galactica, test_gpt2, test_gpt3, test_chatgpt])\n","data_test = datasets.interleave_datasets([test_real, test_fake])\n","\n","\n","\n","# Package dataset in a Dataset container\n","d = Dataset(\n","    id=\"tum-nlp/IDMGSP\",\n","    short_name=\"papers\",\n","    context_train=\"Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.\",\n","    context_test=\"Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.\",\n","    pos_class=\"MACHINE-GENERATED\",\n","    X_train=data_train.to_pandas().head(n_train)[[\"title\", \"abstract\", \"introduction\", \"conclusion\"]].rename(columns={\"title\": \"Title\", \"abstract\": \"Abstract\", \"introduction\": \"Introduction\", \"conclusion\": \"Conclusion\"}),\n","    X_test=data_test.to_pandas().head(n_test)[[\"title\", \"abstract\", \"introduction\", \"conclusion\"]].rename(columns={\"title\": \"Title\", \"abstract\": \"Abstract\", \"introduction\": \"Introduction\", \"conclusion\": \"Conclusion\"}),\n","    y_train=data_train.to_pandas().head(n_train)[\"label\"].map({0: \"HUMAN-WRITTEN\", 1: \"MACHINE-GENERATED\"}),\n","    y_test=data_test.to_pandas().head(n_test)[\"label\"].map({0: \"HUMAN-WRITTEN\", 1: \"MACHINE-GENERATED\"})\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"_1sxQiWHErgs"},"source":["## Fake News"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfvlYYgwEnyK"},"outputs":[],"source":["# Load the training dataset\n","data_train = datasets.load_dataset(\"GonzaloA/fake_news\", split=\"train\")\n","\n","# Randomly shuffle the datasets\n","train_shuffled = data_train.shuffle(seed=seed)\n","\n","# Separate the different classes (fake news and real news)\n","train_fake = train_shuffled.filter(lambda x: x[\"label\"] == 0)\n","train_real = train_shuffled.filter(lambda x: x[\"label\"] == 1)\n","\n","# Create one dataset with alternating fake and real news examples\n","data_train = datasets.interleave_datasets([train_fake, train_real], stopping_strategy=\"first_exhausted\")\n","\n","# Convert to a Pandas DataFrame, split data and label columns, and limit to the training set size\n","df_train = data_train.to_pandas()\n","df_train = df_train.head(n_train)\n","df_train = df_train.rename(columns={\"title\": \"Title\", \"text\": \"Text\"})\n","df_train[\"label\"] = df_train.apply(lambda row: \"FAKE NEWS\" if row[\"label\"] == 0 else \"REAL NEWS\", axis=1)\n","X_train = df_train[[\"Title\", \"Text\"]]\n","y_train = df_train[\"label\"]\n","\n","\n","\n","# Load the test dataset\n","data_test = datasets.load_dataset(\"GonzaloA/fake_news\", split=\"test\")\n","\n","# Randomly shuffle the datasets\n","test_shuffled = data_test.shuffle(seed=seed)\n","\n","# Separate the different classes (fake news and real news)\n","test_fake = test_shuffled.filter(lambda x: x[\"label\"] == 0)\n","test_real = test_shuffled.filter(lambda x: x[\"label\"] == 1)\n","\n","# Create one dataset with alternating fake and real news examples\n","data_test = datasets.interleave_datasets([test_fake, test_real], stopping_strategy=\"first_exhausted\")\n","\n","# Convert to a Pandas DataFrame, split data and label columns, and limit to the test set size\n","df_test = data_test.to_pandas()\n","df_test = df_test.head(n_test)\n","df_test = df_test.rename(columns={\"title\": \"Title\", \"text\": \"Text\"})\n","df_test[\"label\"] = df_test.apply(lambda row: \"FAKE NEWS\" if row[\"label\"] == 0 else \"REAL NEWS\", axis=1)\n","X_test = df_test[[\"Title\", \"Text\"]]\n","y_test = df_test[\"label\"]\n","\n","\n","\n","# Package dataset in a Dataset container\n","d = Dataset(\n","    id=\"GonzaloA/fake_news\",\n","    short_name=\"fake-news-2\",\n","    context_train=\"Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.\",\n","    context_test=\"Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.\",\n","    pos_class=\"FAKE NEWS\",\n","    X_train=X_train,\n","    X_test=X_test,\n","    y_train=y_train,\n","    y_test=y_test\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"unSRgROhYfkL"},"source":["## Hatespeech"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oxxMVu9YhdH"},"outputs":[],"source":["# Load the dataset\n","data = datasets.load_dataset(\"hate_speech18\", split=\"train\")\n","\n","# Remove samples that rely on the context of previous messages\n","data = data.filter(lambda x: x[\"num_contexts\"] == 0)\n","\n","# Randomly shuffle the dataset\n","data_shuffled = data.shuffle(seed=seed)\n","\n","# Separate the different classes (hate speech and no hate speech)\n","data_hate = data_shuffled.filter(lambda x: x[\"label\"] == 1)\n","data_no_hate = data_shuffled.filter(lambda x: x[\"label\"] == 0)\n","\n","# Create one dataset with alternating hate and no hate examples\n","data = datasets.interleave_datasets([data_no_hate, data_hate], stopping_strategy=\"first_exhausted\")\n","\n","# Convert to a pandas dataframe\n","df = data.to_pandas()\n","\n","# Drop unnecessary columns\n","df = df[[\"text\", \"label\"]]\n","\n","# Map the class numbers to natural-language class labels\n","df[\"label\"] = df.apply(lambda row: \"HATE SPEECH\" if row[\"label\"] == 1 else \"NO HATE SPEECH\", axis=1)\n","\n","# Split data and label\n","X = df[\"text\"]\n","y = df[\"label\"]\n","\n","# Split train and test sets\n","X_train = X.iloc[0:n_train]\n","y_train = y.iloc[0:n_train]\n","X_test = X.iloc[n_train:(n_train+n_test)]\n","y_test = y.iloc[n_train:(n_train+n_test)]\n","\n","# Package dataset in a Dataset container\n","d = Dataset(\n","    id=\"hate_speech18\",\n","    short_name=\"hate-speech\",\n","    context_train=\"Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.\",\n","    context_test=\"Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.\",\n","    pos_class=\"HATE SPEECH\",\n","    X_train=X_train,\n","    X_test=X_test,\n","    y_train=y_train,\n","    y_test=y_test\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"A4WeCc7vxzdP"},"source":["## Reviews Amazon"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3iU4Jeax36i"},"outputs":[],"source":["# Download the training dataset\n","print(\"Downloading training dataset ...\")\n","data_train = datasets.load_dataset(\"amazon_polarity\", split=\"train\")\n","\n","# Randomly shuffle the dataset\n","print(\"Preparing training dataset ...\")\n","train_shuffled = data_train.shuffle(seed=seed)\n","\n","# Separate positive and negative reviews\n","train_positive = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 1)\n","train_negative = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 0)\n","\n","# Create one training dataset with alternating examples from both classes\n","df_train = datasets.interleave_datasets([train_positive, train_negative], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n","\n","# Format the dataset\n","X_train = df_train.rename(columns={\"title\": \"Title\", \"content\": \"Content\"})[[\"Title\", \"Content\"]]\n","y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n","\n","\n","\n","# Download the test dataset\n","print(\"Downloading test dataset ...\")\n","data_test = datasets.load_dataset(\"amazon_polarity\", split=\"test\")\n","\n","# Randomly shuffle the dataset\n","print(\"Preparing test dataset ...\")\n","test_shuffled = data_test.shuffle(seed=seed)\n","\n","# Separate positive and negative reviews\n","test_positive = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 1)\n","test_negative = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 0)\n","\n","# Create one test dataset with alternating examples from both classes\n","df_test = datasets.interleave_datasets([test_positive, test_negative], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n","\n","# Format the dataset\n","X_test = df_test.rename(columns={\"title\": \"Title\", \"content\": \"Content\"})[[\"Title\", \"Content\"]]\n","y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n","\n","\n","\n","# Package dataset in a Dataset container\n","print(\"Packaging the dataset ...\")\n","d = Dataset(\n","    id=\"amazon_polarity\",\n","    short_name=\"reviews-amazon\",\n","    context_train=\"Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n","    context_test=\"Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n","    pos_class=\"POSITIVE\",\n","    X_train=X_train,\n","    X_test=X_test,\n","    y_train=y_train,\n","    y_test=y_test\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"hcQTEKt0QK1p"},"source":["## Reviews Yelp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hh4L_ntgQM2m"},"outputs":[],"source":["# Download the training dataset\n","print(\"Downloading training dataset ...\")\n","data_train = datasets.load_dataset(\"yelp_polarity\", split=\"train\")\n","\n","# Randomly shuffle the dataset\n","print(\"Preparing training dataset ...\")\n","train_shuffled = data_train.shuffle(seed=seed)\n","\n","# Separate positive and negative reviews\n","train_positive = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 1)\n","train_negative = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 0)\n","\n","# Create one training dataset with alternating examples from both classes\n","df_train = datasets.interleave_datasets([train_positive, train_negative], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n","\n","# Format the dataset\n","X_train = df_train.rename(columns={\"text\": \"Text\"})[[\"Text\"]]\n","y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n","\n","\n","\n","# Download the test dataset\n","print(\"Downloading test dataset ...\")\n","data_test = datasets.load_dataset(\"yelp_polarity\", split=\"test\")\n","\n","# Randomly shuffle the dataset\n","print(\"Preparing test dataset ...\")\n","test_shuffled = data_test.shuffle(seed=seed)\n","\n","# Separate positive and negative reviews\n","test_positive = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 1)\n","test_negative = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 0)\n","\n","# Create one test dataset with alternating examples from both classes\n","df_test = datasets.interleave_datasets([test_positive, test_negative], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n","\n","# Format the dataset\n","X_test = df_test.rename(columns={\"text\": \"Text\"})[[\"Text\"]]\n","y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n","\n","\n","\n","# Package dataset in a Dataset container\n","print(\"Packaging the dataset ...\")\n","d = Dataset(\n","    id=\"yelp_polarity\",\n","    short_name=\"reviews-yelp\",\n","    context_train=\"Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n","    context_test=\"Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n","    pos_class=\"POSITIVE\",\n","    X_train=X_train,\n","    X_test=X_test,\n","    y_train=y_train,\n","    y_test=y_test\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"ln7LyKru6udZ"},"source":["## Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mW9jy8Yd6w9H"},"outputs":[],"source":["# Download and combine training datasets in different languages\n","train_english = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_arabic = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"arabic\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_french = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_german = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_hindi = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_italian = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_portuguese = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","train_spanish = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n","df_train = datasets.interleave_datasets([train_english, train_arabic, train_french, train_german, train_hindi, train_italian, train_portuguese, train_spanish], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n","\n","# Randomly shuffle the dataset\n","df_train = df_train.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","# Format the dataset\n","X_train = df_train[\"text\"]\n","y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 2: \"POSITIVE\"})\n","\n","\n","\n","# Download and combine test datasets in different languages\n","test_english = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_arabic = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"arabic\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_french = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_german = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_hindi = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_italian = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_portuguese = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","test_spanish = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n","df_test = datasets.interleave_datasets([test_english, test_arabic, test_french, test_german, test_hindi, test_italian, test_portuguese, test_spanish], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n","\n","# Randomly shuffle the dataset\n","df_test = df_test.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","# Format the dataset\n","X_test = df_test[\"text\"]\n","y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 2: \"POSITIVE\"})\n","\n","\n","\n","# Package dataset in a Dataset container\n","d = Dataset(\n","    id=\"cardiffnlp/tweet_sentiment_multilingual\",\n","    short_name=\"sentiment\",\n","    context_train=\"Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.\",\n","    context_test=\"Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.\",\n","    pos_class=\"POSITIVE\",\n","    X_train=X_train,\n","    X_test=X_test,\n","    y_train=y_train,\n","    y_test=y_test\n",")\n","\n","# Add to dataset list\n","dataset_list[d.short_name] = d\n","d"]},{"cell_type":"markdown","metadata":{"id":"ve_kudeg3Yvm"},"source":["# Define Models"]},{"cell_type":"markdown","metadata":{"id":"3kx3M_DLE-tw"},"source":["## FELIX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMxn07AHs40z"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n","from langchain.output_parsers import PydanticOutputParser, StructuredOutputParser, OutputFixingParser, ResponseSchema\n","from langchain.callbacks import get_openai_callback\n","from langchain.embeddings import OpenAIEmbeddings\n","\n","from pydantic import BaseModel, Field\n","from typing import List\n","\n","from hdbscan import HDBSCAN\n","\n","import random\n","import time\n","import json\n","import re\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","\n","from tqdm.notebook import tqdm\n","\n","\n","class PromptingTQDM(tqdm):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}{postfix}]\", *args, **kwargs)\n","        self.reset_stats()\n","\n","    def reset_stats(self):\n","        # Initial stats\n","        self.tokens_in = 0\n","        self.tokens_out = 0\n","        self.costs = 0\n","        self.n_features = 0\n","        self.n_iter = 0\n","\n","    def log_stats(self, tokens_in, tokens_out, cost, n_features):\n","        # Update statistics\n","        self.tokens_in += tokens_in\n","        self.tokens_out += tokens_out\n","        self.costs += cost\n","        self.n_features += n_features\n","        self.n_iter += 1\n","\n","        # Calculate averages\n","        avg_tokens_in = self.tokens_in / self.n_iter\n","        avg_tokens_out = self.tokens_out / self.n_iter\n","        avg_cost = self.costs / self.n_iter\n","\n","        # Update statistics in postfix string\n","        self.set_postfix_str(f\"in {avg_tokens_in:.0f} tokens/{self.unit}, out {avg_tokens_out:.0f} tokens/{self.unit}, {avg_cost:.5f} USD/{self.unit}, {self.costs:.2f} USD total, {self.n_features:.0f} features total\", refresh=True)\n","\n","\n","class NumericalFeature(BaseModel):\n","    name: str = Field(description=\"concise name of the feature\")\n","    zero: str = Field(description=\"meaning of feature value of 0\")\n","    ten: str = Field(description=\"meaning of feature value of 10\")\n","    description: str = Field(description=\"short description of the meaning of this feature\")\n","\n","\n","class NumericalFeatureSet(BaseModel):\n","    features: List[NumericalFeature] = Field(description=\"list of numeric features\")\n","\n","\n","class CategoricalFeature(BaseModel):\n","    name: str = Field(description=\"concise name of the feature\")\n","    possible_values: List[str] = Field(description=\"list of 2-5 different possible values allowed for this feature; the feature can take exactly one of these values at once\")\n","    description: str = Field(description=\"short description of the meaning of this feature\")\n","\n","\n","class CategoricalFeatureSet(BaseModel):\n","    features: List[CategoricalFeature] = Field(description=\"list of categorical features\")\n","\n","\n","# A generic callback class that can be defined for FELIX and is called for every LLM request. Allows tracking of prompts, costs, and token consumption\n","class FELIXCallback:\n","    def __init__(self):\n","        pass\n","\n","    def features_generated_for_pair(self, llm, example_a: str, example_b: str, label_a: str, label_b: str, system_message: str, prompt_message: str, llm_output: str, features: NumericalFeatureSet | CategoricalFeatureSet, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n","        pass\n","\n","    def example_transformed(self, llm, example: str, feature_set: NumericalFeatureSet | CategoricalFeatureSet, system_message: str, prompt_message: str, llm_output: str, scores, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n","        pass\n","\n","    def error_encountered(self, llm, system_message, prompt_message, llm_output, error):\n","        pass\n","\n","\n","class FELIX(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, context=None, llm_name=\"gpt-3.5-turbo\", llm_fallback=\"gpt-3.5-turbo-16k\", llm_generation=None, llm_generation_fallback=None, llm_scoring=None, llm_scoring_fallback=None, llm_embeddings=None, temperature_generation=0.7, temperature_scoring=0.0, discrete_features=True, zero_shot=False, reschuffle_features=False, keep_outlier_features=False, openai_api_key=None, openai_organization=None, callback=None, verbose=False):\n","        self.context = context\n","        self.llm_name = llm_name\n","        self.llm_fallback = llm_fallback\n","        self.llm_generation = llm_generation if llm_generation else llm_name\n","        self.llm_generation_fallback = llm_generation_fallback if llm_generation_fallback else llm_fallback\n","        self.llm_scoring = llm_scoring if llm_scoring else llm_name\n","        self.llm_scoring_fallback = llm_scoring_fallback if llm_scoring_fallback else llm_fallback\n","        self.llm_embeddings = llm_embeddings\n","        self.temperature_generation = temperature_generation\n","        self.temperature_scoring = temperature_scoring\n","        self.discrete_features = discrete_features\n","        self.zero_shot = zero_shot\n","        self.reschuffle_features = reschuffle_features\n","        self.keep_noise = keep_outlier_features\n","        self.openai_api_key = openai_api_key\n","        self.openai_organization = openai_organization\n","        self.callback = callback\n","        self.verbose=verbose\n","\n","        self._validate_class_variables(llm_name=True, context=True, temperature_generation=True, temperature_scoring=True, openai_api_key=True, openai_organization=True, verbose=True)\n","\n","\n","    def save_instance(self, filename):\n","        # Warn the user that certain parameters will not be serialized\n","        if self.callback:\n","            print(\"Warning: Parameter 'callback' is set but cannot be saved to instance.\")\n","        if self._hdbscan:\n","            print(\"Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\")\n","        if self.openai_api_key or self.openai_organization:\n","            print(\"Warning: Parameters 'openai_api_key' and 'openai_organization' will not be saved to instance due to security reasons.\")\n","\n","        # Collect all parameters in a dictionary\n","        data = {\n","            \"context\": self.context,\n","            \"llm_name\": self.llm_name,\n","            \"llm_fallback\": self.llm_fallback,\n","            \"llm_generation\": self.llm_generation,\n","            \"llm_generation_fallback\": self.llm_generation_fallback,\n","            \"llm_scoring\": self.llm_scoring,\n","            \"llm_scoring_fallback\": self.llm_scoring_fallback,\n","            \"llm_embeddings\": self.llm_embeddings,\n","            \"temperature_generation\": self.temperature_generation,\n","            \"temperature_scoring\": self.temperature_scoring,\n","            \"discrete_features\": self.discrete_features,\n","            \"zero_shot\": self.zero_shot,\n","            \"reschuffle_features\": self.reschuffle_features,\n","            \"keep_noise\": self.keep_noise,\n","            \"verbose\": self.verbose,\n","            \"full_feature_set\": self._full_feature_set.json(),\n","            \"features\": self._features.json(),\n","            \"feature_embeddings\": self._feature_embeddings.tolist(),\n","            \"cluster_labels\": self._cluster_labels.tolist(),\n","            \"duplicate_counter\": self._global_duplicate_counter\n","        }\n","\n","        # Convert the dictionary to a JSON string\n","        json_string = json.dumps(data, indent=4)\n","\n","        # Save the JSON to a file\n","        with open(filename, \"w\") as f:\n","            f.write(json_string)\n","\n","\n","    def load_instance(self, filename):\n","        # Load the JSON from the file\n","        with open(filename, \"r\") as f:\n","            data = json.load(f)\n","\n","        # Set all parameters again\n","        self.context = data.get(\"context\")\n","        self.llm_name = data.get(\"llm_name\")\n","        self.llm_fallback = data.get(\"llm_fallback\")\n","        self.llm_generation = data.get(\"llm_generation\")\n","        self.llm_generation_fallback = data.get(\"llm_generation_fallback\")\n","        self.llm_scoring = data.get(\"llm_scoring\")\n","        self.llm_scoring_fallback = data.get(\"llm_scoring_fallback\")\n","        self.llm_embeddings = data.get(\"llm_embeddings\")\n","        self.temperature_generation = data.get(\"temperature_generation\")\n","        self.temperature_scoring = data.get(\"temperature_scoring\")\n","        self.discrete_features = data.get(\"discrete_features\")\n","        self.zero_shot = data.get(\"zero_shot\") if \"zero_shot\" in data else False\n","        self.reschuffle_features = data.get(\"reschuffle_features\") if \"reschuffle_features\" in data else False\n","        self.keep_noise = data.get(\"keep_noise\") if \"keep_noise\" in data else True # Set to True for reverse compatibility (previous versions of FELIX always kept noisy features)\n","        self.verbose = data.get(\"verbose\")\n","        self._full_feature_set = CategoricalFeatureSet.parse_raw(data.get(\"full_feature_set\", [])) if self.discrete_features else NumericalFeatureSet.parse_raw(data.get(\"full_feature_set\", []))\n","        self._features = CategoricalFeatureSet.parse_raw(data.get(\"features\", [])) if self.discrete_features else NumericalFeatureSet.parse_raw(data.get(\"features\", []))\n","        self._feature_embeddings = np.array(data.get(\"feature_embeddings\"))\n","        self._cluster_labels = data.get(\"cluster_labels\")\n","        self._global_duplicate_counter = data.get(\"duplicate_counter\")\n","\n","        # Set parameters to None that cannot be reconstructed\n","        self.callback = None\n","        self._hdbscan = None\n","        self.openai_api_key = None\n","        self.openai_organization = None\n","\n","\n","    def fit(self, X, y=None):\n","        # Validate input parameters and all required class variables\n","        X, y = self._validate_data(X, y)\n","        self._validate_class_variables(llm_name=True, context=True, temperature_generation=True, openai_api_key=True, openai_organization=True)\n","\n","        # Generate features from the examples in X with class labels y\n","        self._full_feature_set = self.generate_features(X, y)\n","\n","        # Consolidate features by removing duplicates\n","        self._features = self.consolidate_features(self._full_feature_set)\n","\n","        return self\n","\n","\n","    def transform(self, X, y=None):\n","        # Validate input parameters and all required class variables\n","        X, y = self._validate_data(X, y, check_y=False)\n","        self._validate_class_variables(llm_name=True, context=True, temperature_scoring=True, openai_api_key=True, openai_organization=True)\n","\n","        # Initialize the LLM for feature scoring\n","        llm = ChatOpenAI(model=self.llm_scoring, temperature=self.temperature_scoring, openai_api_key=self.openai_api_key, openai_organization=self.openai_organization)\n","        llm_fallback = ChatOpenAI(model=self.llm_scoring_fallback, temperature=self.temperature_scoring, openai_api_key=self.openai_api_key, openai_organization=self.openai_organization)\n","\n","        # Score each example in X along the learned features in self._features\n","        X_transformed = []\n","        with PromptingTQDM(total=len(X), desc=\"Scoring examples\", unit=\"prompt\", disable=not self.verbose) as progress_bar:\n","            for i, example in X.items():\n","                scores, cb = self._transform_example(llm, example, self._features, llm_fallback)\n","                X_transformed.append(scores)\n","                if self.reschuffle_features:\n","                    # Randomly reorder the feature set after each scored example, if requested\n","                    random.shuffle(self._features.features)\n","                if self.verbose:\n","                    progress_bar.log_stats(cb.prompt_tokens, cb.completion_tokens, cb.total_cost, len(scores))\n","                    progress_bar.update(1)\n","\n","        # Convert the scores into a Pandas DataFrame\n","        X_transformed = pd.DataFrame(X_transformed)\n","\n","        return X_transformed\n","\n","\n","    def generate_features(self, X, y):\n","        # Validate input parameters and all required class variables\n","        X, y = self._validate_data(X, y)\n","        self._validate_class_variables(llm_name=True, context=True, temperature_generation=True, openai_api_key=True, openai_organization=True)\n","\n","        # Initialize the LLM for feature generation\n","        llm = ChatOpenAI(model=self.llm_generation, temperature=self.temperature_generation, openai_api_key=self.openai_api_key, openai_organization=self.openai_organization)\n","\n","        # Create pairs of examples by iterating over all class combinations and drawing examples from each class\n","        self._pairs = self._generate_pairs(X, y)\n","\n","        # Generate features from the examples in X\n","        self._pairwise_feature_sets = []\n","        with PromptingTQDM(total=len(self._pairs), desc=\"Generating features\", unit=\"prompt\", disable=not self.verbose) as progress_bar:\n","            for example_a, example_b, label_a, label_b in self._pairs:\n","                features, cb = self._generate_features_for_pair(llm, example_a, example_b, label_a, label_b)\n","                self._pairwise_feature_sets.append(features)\n","                if self.verbose:\n","                    progress_bar.log_stats(cb.prompt_tokens, cb.completion_tokens, cb.total_cost, len(features.features))\n","                    progress_bar.update(1)\n","\n","        # Flatten the lists of features generated\n","        flat_list = []\n","        for f in self._pairwise_feature_sets:\n","            flat_list = flat_list + f.features\n","        if self.discrete_features:\n","            full_feature_set = CategoricalFeatureSet(features=flat_list)\n","        else:\n","            full_feature_set = NumericalFeatureSet(features=flat_list)\n","\n","        return full_feature_set\n","\n","\n","    def consolidate_features(self, feature_set):\n","        # Create text embeddings for each feature\n","        self._feature_embeddings = self._create_feature_embeddings(feature_set)\n","\n","        # Cluster the embedded features using HDBSCAN to determine clusters of unique features\n","        self._cluster_labels = self._cluster_features(self._feature_embeddings)\n","\n","        # Handle features identified as noise (i.e., features that were generated only once without another similar feature)\n","        if self.keep_noise:\n","            # Assign all features identified as noise (i.e., not part of a cluster) to their own cluster\n","            next_label = np.max(self._cluster_labels) + 1\n","            for i in range(len(self._cluster_labels)):\n","                if self._cluster_labels[i] == -1:\n","                    self._cluster_labels[i] = next_label\n","                    next_label += 1\n","        else:\n","            # If all features have been identified as noise, put them all in one cluster\n","            if (np.array(self._cluster_labels) == -1).all():\n","                self._cluster_labels = [0 for _ in self._cluster_labels]\n","                if self.verbose:\n","                    print(\"All features identified as noise. Treating them as one cluster\")\n","\n","            # Prune the feature set to only those features that are not identified as noise\n","            feature_set = feature_set.copy(deep=True)\n","            feature_set.features = [f for f, l in zip(feature_set.features, self._cluster_labels) if l != -1]\n","            self._feature_embeddings = np.array([e for e, l in zip(self._feature_embeddings, self._cluster_labels) if l != -1])\n","            self._cluster_labels = np.array([l for l in self._cluster_labels if l != -1])\n","\n","        # Create a consolidated feature set with the most representative feature from each cluster\n","        self._features = self._select_representative_features(feature_set, self._feature_embeddings, self._cluster_labels)\n","\n","        # Add number suffixes to remaining duplicate feature names to make sure that each feature name is unique\n","        self._features = self._ensure_unique_feature_names(self._features)\n","\n","        print(f\"Consolidated to {len(self._features.features)} features ({'incl.' if self.keep_noise else 'excl.'} noise)\")\n","\n","        return self._features\n","\n","\n","    def get_features_as_dataframe(self):\n","        # Validate that features have been learned already and that the learned feature set has the correct type\n","        if not self._features:\n","            raise ValueError(\"No features have been learned yet. Call fit() to learn a set of features.\")\n","        if not isinstance(self._features, CategoricalFeatureSet) and not isinstance(self._features, NumericalFeatureSet):\n","            raise ValueError(f\"Unexpected type '{type(self._features)}' of learned features. Should be either 'CategoricalFeatureSet' or 'NumericalFeatureSet'.\")\n","\n","        # Convert feature set to a Pandas DataFrame\n","        features_json = json.loads(self._features.json())[\"features\"]\n","        df_features = pd.DataFrame(features_json)\n","\n","        # Rename the DataFrame columns in a more readable format\n","        df_features.columns = df_features.columns.str.replace('_', ' ').str.title()\n","\n","        return df_features\n","\n","\n","    def _validate_class_variables(self, llm_name=False, context=False, temperature_generation=False, temperature_scoring=False, openai_api_key=False, openai_organization=False, verbose=False):\n","        if temperature_generation:\n","            if not isinstance(self.temperature_generation, float):\n","                raise ValueError(\"temperature_generation must be of type float.\")\n","            if not 0.0 <= self.temperature_generation <= 1.0:\n","                raise ValueError(\"temperature_generation must lie between 0.0 and 1.0.\")\n","\n","        if temperature_scoring:\n","            if not isinstance(self.temperature_scoring, float):\n","                raise ValueError(\"temperature_scoring must be of type float.\")\n","            if not 0.0 <= self.temperature_scoring <= 1.0:\n","                raise ValueError(\"temperature_scoring must lie between 0.0 and 1.0.\")\n","\n","        if verbose:\n","            if not isinstance(self.verbose, bool):\n","                raise ValueError(f\"verbose must be of type bool but is of type {type(self.verbose)}.\")\n","\n","        return True\n","\n","\n","    def _validate_data(self, X, y, check_X=True, check_y=True):\n","        if check_X:\n","            # Check if X is array-like\n","            if not (isinstance(X, (list, np.ndarray, pd.Series, pd.DataFrame))):\n","                raise ValueError(\"X must be an array-like object.\")\n","\n","            # Check if X has 1-2 dimensions\n","            if isinstance(X, (list, np.ndarray)):\n","                if len(np.shape(X)) > 2:\n","                    raise ValueError(\"X must have 1 or 2 dimensions.\")\n","            elif isinstance(X, (pd.Series, pd.DataFrame)):\n","                if len(X.shape) > 2:\n","                    raise ValueError(\"X must have 1 or 2 dimensions.\")\n","\n","            # Check if X is empty\n","            if len(X) == 0:\n","                raise ValueError(\"X cannot be empty.\")\n","\n","            # Make sure X is a Pandas DataFrame\n","            X = pd.DataFrame(X)\n","\n","            # Serialize DataFrame X to a 1-dimensional Series of text data\n","            X = self._serialize_dataframe(X)\n","\n","        if check_y:\n","            # Check if y is array-like\n","            if not (isinstance(y, (list, np.ndarray, pd.Series, pd.DataFrame))):\n","                raise ValueError(\"y must be an array-like object.\")\n","\n","            # Check if y has exactly 1 dimension\n","            if isinstance(y, (list, np.ndarray)):\n","                if len(np.shape(y)) != 1:\n","                    raise ValueError(\"y can only have 1 dimension.\")\n","            elif isinstance(y, (pd.Series, pd.DataFrame)):\n","                if len(y.shape) != 1:\n","                    raise ValueError(\"y can only have 1 dimension.\")\n","\n","            # Check if y is empty\n","            if len(y) == 0:\n","                raise ValueError(\"y cannot be empty.\")\n","\n","            # Make sure y is a Pandas object\n","            y = pd.Series(y)\n","\n","            # Check if y contains at least two unique class labels\n","            if y.nunique() < 2:\n","                raise ValueError(f\"y must contain at least two unique values but only has {y.nunique()} unique value.\")\n","\n","        if check_X and check_y:\n","            # Check if X and y have the same length\n","            if len(X) != len(y):\n","                raise ValueError(f\"X and y must have the same length but X has length {len(X)} while y has length {len(y)}.\")\n","\n","        return X, y\n","\n","\n","    def _serialize_dataframe(self, df):\n","        # Check if df is a Pandas DataFrame\n","        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n","            raise ValueError(\"df must be a Pandas DataFrame or Series.\")\n","\n","        # If df is already a Pandas Series, return it\n","        if isinstance(df, pd.Series):\n","            return df\n","\n","        # Serialize DataFrame df to a 1-dimensional Series of text data\n","        if len(df.columns) == 1:\n","            # If df is a single-column DataFrame, keep it as a string Series\n","            return df[df.columns[0]].astype(str)\n","        else:\n","            # If Process DataFrame with multiple columns into the required format\n","            def serialize_row(row):\n","                # Replace NaN values with a placeholder, and cast each value to string\n","                items = [(col, str(val) if not pd.isnull(val) else \"N/A\") for col, val in row.items()]\n","                return \"\\n\\n\".join([f\"{col}: {val}\" for col, val in items])\n","            return df.apply(serialize_row, axis=1)\n","\n","\n","    def _generate_pairs(self, X, y):\n","        # Determine the set of unique classes\n","        classes = y.unique()\n","\n","        # Create pairs of examples by iterating over all class combinations and drawing examples from each class\n","        pairs = []\n","        for i in range(len(classes)):\n","            for j in range(i+1, len(classes)):\n","                class_a = classes[i]\n","                class_b = classes[j]\n","                examples_class_a = X[y == class_a]\n","                examples_class_b = X[y == class_b]\n","\n","                # Create pairs by iterating over the drawn examples from both classes.\n","                for k in range(max(len(examples_class_a), len(examples_class_b))):\n","                    # If one class runs out of examples, start with the first example of that class again. Alternate the order of the examples (AB, BA, AB, ...) (hence, the k % 2)\n","                    pairs.append((\n","                        examples_class_a.iloc[k % len(examples_class_a)] if k % 2 == 0 else examples_class_b.iloc[k % len(examples_class_b)],\n","                        examples_class_b.iloc[k % len(examples_class_b)] if k % 2 == 0 else examples_class_a.iloc[k % len(examples_class_a)],\n","                        class_a if k % 2 == 0 else class_b,\n","                        class_b if k % 2 == 0 else class_a\n","                    ))\n","\n","        return pairs\n","\n","\n","    def _generate_features_for_pair(self, llm, example_a, example_b, label_a, label_b):\n","        # Check if any input parameters are None\n","        if llm == None:\n","            raise ValueError(\"llm cannot be None.\")\n","        if example_a == None:\n","            raise ValueError(\"example_a cannot be None.\")\n","        if example_b == None:\n","            raise ValueError(\"example_b cannot be None.\")\n","        if label_a == None:\n","            raise ValueError(\"label_a cannot be None.\")\n","        if label_b == None:\n","            raise ValueError(\"label_b cannot be None.\")\n","\n","        # Check if llm is an OpenAIChat model\n","        if not isinstance(llm, ChatOpenAI):\n","            raise ValueError(f\"llm must be of type langchain.chat_models.openai.ChatOpenAI but found type {type(llm)}.\")\n","\n","        # Check if example_a and example_b are strings\n","        if not isinstance(example_a, str):\n","            raise ValueError(f\"example_a must be of type str but found type {type(example_a)}.\")\n","        if not isinstance(example_b, str):\n","            raise ValueError(f\"example_b must be of type str but found type {type(example_b)}.\")\n","\n","        # Check if label_a and label_b are different\n","        if label_a == label_b:\n","            raise ValueError(f\"label_a and label_b must be different (meaning that example_a and example_b must be from different classes) but found label_a=label_b={label_a}\")\n","\n","        # Check if all required class variables are set correctly\n","        self._validate_class_variables(context=True)\n","\n","        # Define the output schema and parser (OutputFixingParser will try to fix the LLM output in case the PydanticOutputParser fails)\n","        if self.discrete_features:\n","            output_parser = OutputFixingParser.from_llm(\n","                parser=PydanticOutputParser(pydantic_object=CategoricalFeatureSet),\n","                llm=llm\n","            )\n","        else:\n","            output_parser = OutputFixingParser.from_llm(\n","                parser=PydanticOutputParser(pydantic_object=NumericalFeatureSet),\n","                llm=llm\n","            )\n","\n","        # Define the prompt template for the system message with the context and overall task instructions\n","        system_template = SystemMessagePromptTemplate(prompt=PromptTemplate(\n","            template=\"You are a data scientist. Your goal is to engineer a set of features suitable to distinguish examples from different classes.{context}\",\n","            input_variables=[],\n","            partial_variables={\n","                \"context\": \" The concrete context is the following: \" + self.context if self.context and self.context != \"\" else \"\"\n","            }\n","        ))\n","\n","        # Define the prompt template for with the instructions to generate features\n","        if self.zero_shot:\n","            generation_template = HumanMessagePromptTemplate(prompt=PromptTemplate(\n","                template=\"\"\"\\\n","##### Instructions #####\\n\\n\\\n","Create a list of features that would best distinguish examples of class '{label_a}' from examples of class '{label_b}'. \\\n","Features can be anything from very specific (e.g., occurrences of a specific word) to very abstract (e.g., describing the logical flow of text). However, features should generalize even to new and previously unseen examples that may come from a slightly different domain. \\\n","{feature_instructions}\\n\\n\\\n","{format_instructions}\"\"\",\n","                input_variables=[\"label_a\", \"label_b\"],\n","                partial_variables={\n","                    \"feature_instructions\": \"\"\"Each feature should be categorical with at least 2 but no more than 5 different possible values. \\\n","Possible values should be words but not numbers. \\\n","For any given example, each feature should unambiguously take exactly one value out of the list of possible values. \\\n","Thus, avoid features that could take two or more values for any example and ensure that there is always exactly one value that fits an example (if necessary, include fallback values such as 'Other', 'Neutral', or 'Not applicable').\"\"\" if self.discrete_features else \"\"\"\\\n","Each feature should be numeric and measured on a scale of 0 to 10. Therefore, indicate for each feature what 0 and 10 mean, respectively (e.g., 0 = \"few\", 10 = \"many\").\"\"\",\n","                    \"format_instructions\": output_parser.get_format_instructions()\n","                }\n","            ))\n","        else:\n","            generation_template = HumanMessagePromptTemplate(prompt=PromptTemplate(\n","                template=\"\"\"\\\n","##### {class_intro} '{label_a}' #####\\n\\n\\\n","{example_a}\\n\\n\\n\\\n","##### {class_intro} '{label_b}' #####\\n\\n\\\n","{example_b}\\n\\n\\n\\\n","##### Instructions #####\\n\\n\\\n","Based on the above examples and their classes, create a list of features that would best distinguish examples of class '{label_a}' from examples of class '{label_b}'. \\\n","Features can be anything from very specific (e.g., occurrences of a specific word) to very abstract (e.g., describing the logical flow of text). However, features should generalize even to new and previously unseen examples that may come from a slightly different domain. \\\n","{feature_instructions}\\n\\n\\\n","{format_instructions}\"\"\",\n","                input_variables=[\"label_a\", \"label_b\", \"example_a\", \"example_b\"],\n","                partial_variables={\n","                    \"class_intro\": \"The following example is from class\",\n","                    \"feature_instructions\": \"\"\"Each feature should be categorical with at least 2 but no more than 5 different possible values. \\\n","Possible values should be words but not numbers. \\\n","For any given example, each feature should unambiguously take exactly one value out of the list of possible values. \\\n","Thus, avoid features that could take two or more values for any example and ensure that there is always exactly one value that fits an example (if necessary, include fallback values such as 'Other', 'Neutral', or 'Not applicable').\"\"\" if self.discrete_features else \"\"\"\\\n","Each feature should be numeric and measured on a scale of 0 to 10. Therefore, indicate for each feature what 0 and 10 mean, respectively (e.g., 0 = \"few\", 10 = \"many\").\"\"\",\n","                    \"format_instructions\": output_parser.get_format_instructions()\n","                }\n","            ))\n","\n","        # Assemble a list of chat messages\n","        messages = [\n","            system_template.format(),\n","            generation_template.format(label_a=label_a, label_b=label_b) if self.zero_shot else generation_template.format(label_a=label_a, label_b=label_b, example_a=example_a, example_b=example_b)\n","        ]\n","\n","        # Execute the prompt and parse the output\n","        with get_openai_callback() as cb:\n","            output = llm(messages).content\n","            features = output_parser.parse(output)\n","\n","            # Convert all feature names to lower-case and replace spaces with underscores\n","            for f in features.features:\n","                f.name = f.name.lower().replace(\" \", \"_\")\n","\n","            # Report this LLM request to the callback for tracking and evaluation\n","            if self.callback:\n","                self.callback.features_generated_for_pair(llm, example_a, example_b, label_a, label_b, messages[0].content, messages[1].content, output, features, cb.total_cost, cb.total_tokens, cb.prompt_tokens, cb.completion_tokens)\n","\n","            return features, cb\n","\n","\n","    def _create_feature_embeddings(self, feature_set):\n","        # Check if feature_set is of type FeatureSet\n","        if self.discrete_features and not isinstance(feature_set, CategoricalFeatureSet):\n","            raise ValueError(f\"feature_set must be of type CategoricalFeatureSet but found {type(feature_set)}.\")\n","        elif not self.discrete_features and not isinstance(feature_set, NumericalFeatureSet):\n","            raise ValueError(f\"feature_set must be of type NumericalFeatureSet but found {type(feature_set)}.\")\n","        for f in feature_set.features:\n","            if self.discrete_features:\n","                if not isinstance(f, CategoricalFeature):\n","                    raise ValueError(f\"All features in feature_set.features must be of type CategoricalFeature but found an element of type {type(f)}: {f}.\")\n","            else:\n","                if not isinstance(f, NumericalFeature):\n","                    raise ValueError(f\"All features in feature_set.features must be of type NumericalFeature but found an element of type {type(f)}: {f}.\")\n","\n","        # Check if feature_set contains at least one feature\n","        if len(feature_set.features) == 0:\n","            raise ValueError(f\"feature_set must contain at least 1 feature but found {len(feature_set.features)} features.\")\n","\n","        # Check if all required class variables are set correctly\n","        self._validate_class_variables(openai_api_key=True, openai_organization=True)\n","\n","        # Instantiate OpenAI embeddings\n","        embeddings = OpenAIEmbeddings(\n","            model=self.llm_embeddings,\n","            openai_api_key=self.openai_api_key,\n","            openai_organization=self.openai_organization\n","        ) if self.llm_embeddings else OpenAIEmbeddings(\n","            openai_api_key=self.openai_api_key,\n","            openai_organization=self.openai_organization\n","        )\n","\n","        # Create text embeddings for each feature\n","        with tqdm(total=1, desc=\"Creating feature embeddings\", disable=not self.verbose) as progress_bar:\n","            feature_embeddings = embeddings.embed_documents([f.json() for f in feature_set.features])\n","            feature_embeddings = np.array(feature_embeddings)\n","            if self.verbose:\n","                progress_bar.update(1)\n","\n","        return feature_embeddings\n","\n","\n","    def _cluster_features(self, feature_embeddings):\n","        # Check if feature_embeddings has the right format\n","        if not isinstance(feature_embeddings, np.ndarray):\n","            raise ValueError(f\"feature_embeddings must be of type np.ndarray but found {type(feature_embeddings)}.\")\n","\n","        # Cluster embedded features using HDBSCAN to determine clusters of unique features\n","        with tqdm(total=1, desc=\"Clustering features\", disable=not self.verbose) as progress_bar:\n","            np.random.seed(0)\n","            self._hdbscan = HDBSCAN(min_cluster_size=2, allow_single_cluster=True, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0)\n","            cluster_labels = self._hdbscan.fit_predict(feature_embeddings)\n","            if self.verbose:\n","                progress_bar.update(1)\n","\n","        return cluster_labels\n","\n","\n","    def _select_representative_features(self, feature_set, feature_embeddings, cluster_labels):\n","        # Find the unique cluster labels\n","        unique_labels = np.unique(cluster_labels)\n","\n","        # Initialize a list to hold the representative points of each cluster\n","        representative_features = []\n","\n","        # Loop over each unique cluster label\n","        for label in unique_labels:\n","            # Get the points in this cluster\n","            cluster_points = np.array([(point, cluster_label, feature) for point, cluster_label, feature in zip(feature_embeddings, cluster_labels, feature_set.features) if cluster_label == label], dtype=object)\n","\n","            # Calculate the centroid of these points\n","            centroid = np.mean([p[0] for p in cluster_points], axis=0)\n","\n","            # Calculate the distance of each point from the centroid\n","            distances = cdist([centroid], [p[0] for p in cluster_points])[0]\n","\n","            # Get the index of the closest point\n","            closest_point_idx = np.argmin(distances)\n","\n","            # Get the closest point and add it to our list\n","            representative_features.append(cluster_points[closest_point_idx][2])\n","\n","        # Consolidate all representative features into a new feature set\n","        if self.discrete_features:\n","            feature_set_consolidated = CategoricalFeatureSet(features=representative_features)\n","        else:\n","            feature_set_consolidated = NumericalFeatureSet(features=representative_features)\n","\n","        return feature_set_consolidated\n","\n","\n","    def _ensure_unique_feature_names(self, feature_set):\n","        # Convert all feature names to lower-case and replace spaces with underscores\n","        for f in feature_set.features:\n","            f.name = f.name.lower().replace(\" \", \"_\")\n","\n","        # Count the number of duplicate feature names in the feature set\n","        self._global_duplicate_counter = 0\n","\n","        # Iterate over all feature names and look for duplicates. If a duplicate is found, attach a number suffix (\"_2\", \"_3\", ...)\n","        for i in range(len(feature_set.features)):\n","            local_duplicate_counter = 0\n","            for j in range(i+1, len(feature_set.features)):\n","                if feature_set.features[i].name == feature_set.features[j].name:\n","                    self._global_duplicate_counter += 1\n","                    local_duplicate_counter += 1\n","                    feature_set.features[j].name = feature_set.features[j].name + \"_\" + str(local_duplicate_counter + 1)\n","\n","        return feature_set\n","\n","\n","    def _transform_example(self, llm, example, feature_set, llm_fallback=None):\n","        # Define the response schemas to get a score for each feature\n","        response_schemas = []\n","        for f in feature_set.features:\n","            response_schemas.append(ResponseSchema(\n","                name=f.name.lower().replace(\" \", \"_\"),\n","                description=f\"'{f.description}' (value can be any of {str(f.possible_values)})\",\n","                type=\"string\"\n","            ) if self.discrete_features else ResponseSchema(\n","                name=f.name.lower().replace(\" \", \"_\"),\n","                description=f\"'{f.name.lower()}' (value from 0 to 10, 0 means '{f.zero}', 10 means '{f.ten}')\",\n","                type=\"int\"\n","            ))\n","\n","        # Define the output schema and parser (OutputFixingParser will try to fix the LLM output in case the StructuredOutputParser fails)\n","        output_parser = OutputFixingParser.from_llm(\n","            parser=StructuredOutputParser.from_response_schemas(response_schemas),\n","            llm=llm_fallback if llm_fallback else llm # Note: Use fallback LLM (with larger context window) to be sure that output fixing does not fail because of a too small context window\n","        )\n","\n","        # Define the prompt template for the system message with the context and overall task instructions\n","        system_template = SystemMessagePromptTemplate(prompt=PromptTemplate(\n","            template=\"You are a data annotator. Your task is to annotate a given example by scoring it with regards to several criteria.{context}\",\n","            input_variables=[],\n","            partial_variables={\n","                \"context\": \" The concrete context is the following: \" + self.context if self.context and self.context != \"\" else \"\"\n","            }\n","        ))\n","\n","        # Define the prompt template with the instructions to score an example along a set of features\n","        score_template = HumanMessagePromptTemplate(prompt=PromptTemplate(\n","            template=\"\"\"\\\n","##### Here is an example that you should annotate #####\\n\\n\n","{example}\\n\\n\\n\\\n","##### Instructions #####\\n\\n\\\n","{instructions}\\n\\n\\\n","{format_instructions}\\n\\n\n","Do not add any comments (starting with //) inside the JSON!\"\"\",\n","            input_variables=[\"example\"],\n","            partial_variables={\n","                \"instructions\": \"Score the above example along the following list of criteria. For each criteria, assign \" + (\"one of the possible values\" if self.discrete_features else \"an integer value from 0 to 10\") + \" that most accurately describes the example.\",\n","                \"format_instructions\": output_parser.get_format_instructions()\n","            }\n","        ))\n","\n","        # Assemble a list of chat messages\n","        messages = [\n","            system_template.format(),\n","            score_template.format(example=example, format_instructions=output_parser.get_format_instructions())\n","        ]\n","\n","        # Execute the prompt and parse the output\n","        with get_openai_callback() as cb:\n","            try:\n","                output = llm(messages).content\n","                output = self._remove_json_comments(output)\n","                scores = output_parser.parse(output)\n","                used_fallback = False\n","            except Exception as e:\n","                # Report the exception to the callback for later analysis\n","                if self.callback:\n","                    self.callback.error_encountered(llm, messages[0].content, messages[1].content, output, e)\n","\n","                # If the request is invalid (often because the LLM's context limit is exceeded), retry with the fallback model\n","                used_fallback = True\n","                print(f\"Unexpected error occured using '{llm.model_name}': {str(e)}\")\n","                scores = None\n","                if llm_fallback:\n","                    print(f\"Retry using fallback model '{llm_fallback.model_name}'\")\n","                    try:\n","                        output = llm_fallback(messages).content\n","                        output = self._remove_json_comments(output)\n","                        scores = output_parser.parse(output)\n","                    except Exception as e2:\n","                        # Report the exception to the callback for later analysis\n","                        if self.callback:\n","                            self.callback.error_encountered(llm_fallback, messages[0].content, messages[1].content, output, e)\n","\n","                        print(f\"Retry with fallback model '{llm_fallback.model_name}' unsuccessful: {str(e2)}\")\n","\n","            if scores:\n","                # Validate all scores (i.e., make sure that only allowed feature values are assigned)\n","                if self.discrete_features:\n","                    # Validate categorical features\n","                    for name, value in scores.items():\n","                        i = [f.name for f in feature_set.features].index(name)\n","                        allowed_values = feature_set.features[i].possible_values\n","                        # Replace all non-compliant values with 'NaN'\n","                        if value not in allowed_values:\n","                            scores[name] = np.nan\n","                            # print(f\"Replaced non-compliant value '{value}' with 'NaN' for feature '{name}' with allowed values {allowed_values}.\")\n","                else:\n","                    # Validate numeric feature values\n","                    for name, value in scores.items():\n","                        try:\n","                            numeric_value = float(value)                    # Try to convert to a float first (catches strings that can't be converted to numbers)\n","                            numeric_value = round(numeric_value)            # Now round to the nearest integer\n","                            if numeric_value < 0 or numeric_value > 10:\n","                                # If the value is not in the allowed range, throw an error\n","                                raise ValueError(\"Value must be in range 0-10!\")\n","                        except ValueError:\n","                            # If a feature value is not compliant, replace it with 'NaN'\n","                            scores[name] = np.nan\n","            else:\n","                # If scoring failed, return a dictionary with empty scores\n","                scores = {}\n","                for feature in response_schemas:\n","                    scores[feature.name] = None\n","\n","            # Report this LLM request to the callback for tracking and evaluation\n","            if self.callback:\n","                self.callback.example_transformed(llm_fallback if used_fallback else llm, example, feature_set, messages[0].content, messages[1].content, output, scores, cb.total_cost, cb.total_tokens, cb.prompt_tokens, cb.completion_tokens)\n","\n","            return scores, cb\n","\n","\n","    def _remove_json_comments(self, json_str):\n","        # Remove all comments from JSON strings sometimes returned by GPT-4-Turbo as they often prevent parsing\n","        pattern = r'//.*?$'\n","        cleaned_json_str = re.sub(pattern, '', json_str, flags=re.MULTILINE)\n","        return cleaned_json_str"]},{"cell_type":"markdown","metadata":{"id":"_nm7KMw8X_8w"},"source":["## Zero-Shot GPT Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T51by2nXVWt"},"outputs":[],"source":["from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n","from sklearn.utils.multiclass import unique_labels\n","\n","from tqdm.notebook import tqdm\n","\n","from datetime import datetime\n","import numpy as np\n","\n","\n","class PromptingTQDM(tqdm):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}{postfix}]\", *args, **kwargs)\n","        self.reset_stats()\n","\n","    def reset_stats(self):\n","        # Initial stats\n","        self.tokens_in = 0\n","        self.tokens_out = 0\n","        self.costs = 0\n","        self.n_features = 0\n","        self.n_iter = 0\n","\n","    def log_stats(self, tokens_in, tokens_out, cost, n_features):\n","        # Update statistics\n","        self.tokens_in += tokens_in\n","        self.tokens_out += tokens_out\n","        self.costs += cost\n","        self.n_features += n_features\n","        self.n_iter += 1\n","\n","        # Calculate averages\n","        avg_tokens_in = self.tokens_in / self.n_iter\n","        avg_tokens_out = self.tokens_out / self.n_iter\n","        avg_cost = self.costs / self.n_iter\n","\n","        # Update statistics in postfix string\n","        self.set_postfix_str(f\"in {avg_tokens_in:.0f} tokens/{self.unit}, out {avg_tokens_out:.0f} tokens/{self.unit}, {avg_cost:.5f} USD/{self.unit}, {self.costs:.2f} USD total, {self.n_features:.0f} features total\", refresh=True)\n","\n","\n","\n","class ZeroShotGPTCallback:\n","\n","    def __init__(self):\n","        self.consumption_log = []\n","\n","\n","    def log_consumption(self, llm, example: str, system_message: str, prompt_message: str, llm_output: str, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n","        self.consumption_log.append({\n","            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n","            \"LLM\": llm.model_name,\n","            \"Temperature\": llm.temperature,\n","            \"Example\": example,\n","            \"System Message\": system_message,\n","            \"Prompt Message\": prompt_message,\n","            \"LLM Output\": llm_output,\n","            \"Total Cost\": total_cost,\n","            \"Total Tokens\": total_tokens,\n","            \"Prompt Tokens\": prompt_tokens,\n","            \"Completion Tokens\": completion_tokens\n","        })\n","\n","\n","\n","class ZeroShotGPTClassifier(BaseEstimator, ClassifierMixin):\n","\n","    def __init__(self, context=None, llm_name=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=None, openai_organization=None, callback: ZeroShotGPTCallback = None, verbose=False):\n","        self.context = context\n","        self.llm_name = llm_name\n","        self.temperature = temperature\n","        self.openai_api_key = openai_api_key\n","        self.openai_organization = openai_organization\n","        self.callback = callback\n","        self.verbose=verbose\n","\n","        # Initialize the LLM for feature scoring\n","        self.llm = ChatOpenAI(model=self.llm_name, temperature=self.temperature, openai_api_key=self.openai_api_key, openai_organization=self.openai_organization)\n","\n","        # Define the prompt template for the system message with the context and overall task instructions\n","        self.system_template = SystemMessagePromptTemplate(prompt=PromptTemplate(\n","            template=\"You are a data classifier. Your task is to classify a given example into one of multiple classes.{context}\",\n","            input_variables=[],\n","            partial_variables={\n","                \"context\": \" The concrete context is the following: \" + self.context if self.context and self.context != \"\" else \"\"\n","            }\n","        ))\n","\n","        # Define the prompt template with the instructions to classify an example\n","        self.classification_template = HumanMessagePromptTemplate(prompt=PromptTemplate(\n","            template=\"\"\"\\\n","##### Here is an example that you should classify #####\\n\\n\n","{example}\\n\\n\\n\\\n","##### Instructions #####\\n\\n\\\n","{instructions} {classes}.\"\"\",\n","            input_variables=[\"example\", \"classes\"],\n","            partial_variables={\n","                \"instructions\": \"What is the most likely class of this example? Respond only with exactly one of the following class names and nothing else:\"\n","            }\n","        ))\n","\n","\n","    def fit(self, X, y):\n","        # Store the classes and data seen during fit\n","        self.classes_ = unique_labels(y)\n","        self.X_ = X\n","        self.y_ = y\n","\n","        # Return the classifier\n","        return self\n","\n","\n","    def predict(self, X):\n","        # Check if fit has been called\n","        check_is_fitted(self)\n","\n","        # Use the data validation and preparation functionality implemented in FELIX\n","        temp_felix = FELIX()\n","        X, y = temp_felix._validate_data(X, None, check_y=False)\n","\n","        # Classify each example in X\n","        predictions = []\n","        with PromptingTQDM(total=len(X), desc=\"Classifying examples\", unit=\"prompt\", disable=not self.verbose) as progress_bar:\n","            for i, example in X.items():\n","                with get_openai_callback() as cb:\n","                    # Assemble a list of chat messages\n","                    messages = [\n","                        self.system_template.format(),\n","                        self.classification_template.format(example=example, classes=\", \".join([f\"'{str(c)}'\" for c in self.classes_]))\n","                    ]\n","\n","                    # Execute the prompt and parse the output\n","                    prediction = self.llm(messages).content\n","                    if prediction in self.classes_:\n","                        predictions.append(prediction)\n","                    else:\n","                        print(f\"Warning: prediction '{prediction}' is not a valid class.\")\n","                        predictions.append(np.nan)\n","\n","                    # Update the progress bar if self.verbose=True\n","                    if self.verbose:\n","                        progress_bar.log_stats(cb.prompt_tokens, cb.completion_tokens, cb.total_cost, 1)\n","                        progress_bar.update(1)\n","\n","                    # Report this LLM request to the callback for tracking and evaluation\n","                    if self.callback and isinstance(self.callback, ZeroShotGPTCallback):\n","                        self.callback.log_consumption(self.llm, example, messages[0].content, messages[1].content, prediction, cb.total_cost, cb.total_tokens, cb.prompt_tokens, cb.completion_tokens)\n","\n","        return predictions"]},{"cell_type":"markdown","source":["## Fine-Tuned LLM"],"metadata":{"id":"30UBAYwNAcE1"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.utils.multiclass import unique_labels\n","\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","from transformers import Trainer, TrainingArguments, set_seed\n","import torch\n","\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Class which iteratively returns a model input string as a tensor\n","class TorchDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor([self.labels[idx]])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","\n","class FineTunedClassificationLLM(BaseEstimator, ClassifierMixin):\n","\n","    def __init__(self, model_name=\"FacebookAI/roberta-base\", max_tokens=512, epochs=1, model_path=\"./models/\", output_path=\"./results\", logging_path=\"./logs\", seed=42):\n","        self.model_name = model_name\n","        self.max_tokens = max_tokens\n","        self.epochs = epochs\n","        self.model_path = model_path\n","        self.output_path = output_path\n","        self.logging_path = logging_path\n","        self.seed = seed\n","\n","        self.training_args = TrainingArguments(\n","            seed = self.seed,                   # Random seed for initialization\n","            output_dir=self.output_path,        # Directory for storing model predictions and checkpoints\n","            logging_dir=self.logging_path,      # Directory for storing Tensorboard logs\n","            num_train_epochs=self.epochs,       # Number of training epochs to perform\n","            per_device_train_batch_size=16,     # Batch size per GPU/TPU core/CPU for training\n","            learning_rate = 5e-5,               # Initial learning rate for Adam\n","            fp16 = True,                        # Use 16-bit (mixed) precision training (through NVIDIA apex)\n","        )\n","\n","        if not torch.cuda.is_available():\n","            print(\"Warning: Cuda is not available on this hardware. Training an LLM may run into problems.\")\n","\n","        # Set the seed for model initialization (for reproducible initial weights of the fully-connected classification layer)\n","        set_seed(self.seed)\n","\n","        # Load model and tokenizer\n","        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=2).to(\"cuda\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","\n","\n","    def fit(self, X, y):\n","        # Remember the unique class labels\n","        self.classes_ = list(unique_labels(y))\n","        if len(self.classes_) != 2:\n","            raise ValueError(f\"FineTunedClassificationLLM only supports binary classification but found {len(self.classes_)} unique classes.\")\n","\n","        # Map class labels to 0 and 1\n","        y = [self.classes_.index(label) for label in y]\n","\n","        # Convert input data to list, if necessary (e.g., when data is provided as a Pandas Series)\n","        if not isinstance(X, list):\n","            X = list(X)\n","\n","        # Tokenize the data\n","        train_encodings = self.tokenizer(X, padding=True, truncation=True, max_length=self.max_tokens)\n","\n","        # Convert tokenized data into a torch Dataset\n","        train_dataset = TorchDataset(train_encodings, y)\n","\n","        # Train the model\n","        trainer = Trainer(\n","            model=self.model,                    # the instantiated Transformers model to be trained\n","            args=self.training_args,             # training arguments which are defined above\n","            train_dataset=train_dataset,         # training dataset\n","        )\n","        trainer.train()\n","\n","        # Save the model\n","        self.model.save_pretrained(self.model_path)\n","        self.tokenizer.save_pretrained(self.model_path)\n","\n","\n","    def predict(self, X):\n","        preds = []\n","        for x in X:\n","            # Tokenize the example\n","            inputs = self.tokenizer(x, padding=True, truncation=True, max_length=self.max_tokens, return_tensors=\"pt\").to(\"cuda\")\n","\n","            # Perform inference with the model\n","            outputs = self.model(**inputs)\n","\n","            # Convert into the predicted class label\n","            pred = self.classes_[outputs[0].softmax(1).argmax()]\n","            preds.append(pred)\n","\n","        return preds"],"metadata":{"id":"PcItsccUAf7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRfT8MeGncGM"},"source":["## Data Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYQ3E2dtqPCm"},"outputs":[],"source":["# Imports for embedding transformation\n","from langchain.embeddings import OpenAIEmbeddings\n","\n","# Imports for TF-IDF transformation\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk import word_tokenize\n","import nltk\n","\n","# Imports for imputation of missing values\n","from sklearn.impute import KNNImputer\n","\n","\n","class DataTransformer:\n","\n","    def __init__(self, dataset: Dataset):\n","        self.dataset = dataset\n","        self.last_instance = None # Reference to the last trained model instance\n","\n","\n","    def get_raw(self):\n","        # Use transformation functionality already implemented in FELIX\n","        serializer = FELIX()\n","\n","        # Convert multiple-column data into single-text data first\n","        X_train_raw = serializer._serialize_dataframe(self.dataset.X_train)\n","        X_test_raw = serializer._serialize_dataframe(self.dataset.X_test)\n","\n","        # Name the Pandas Series\n","        X_train_raw.name = \"Raw Text\"\n","        X_test_raw.name = \"Raw Text\"\n","\n","        self.last_instance = None\n","\n","        return X_train_raw, X_test_raw\n","\n","\n","    def get_tfidf(self):\n","        # Convert multiple-column data into single-text data first\n","        X_train_raw, X_test_raw = self.get_raw()\n","\n","        # Ensure the necessary NLTK data is downloaded\n","        nltk.download(\"punkt\")\n","        nltk.download(\"stopwords\")\n","\n","        # Initialize stemmer\n","        stemmer = PorterStemmer()\n","\n","        # Get the list of stop words from NLTK\n","        stop_words = set(stopwords.words(\"english\"))\n","\n","        # Custom tokenizer function using NLTK's word_tokenize, a stop words list, and a stemmer\n","        def tokenize_and_stem(text):\n","            tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]  # Remove non-alphabetic tokens\n","            filtered_tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n","            return [stemmer.stem(t) for t in filtered_tokens]  # Stemming\n","\n","        # Initialize the vectorizer\n","        vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem)\n","\n","        # Calculate the TF-IDF representation of the raw text\n","        X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n","        X_test_tfidf = vectorizer.transform(X_test_raw)  # Note: use transform() not fit_transform() for test data\n","        self.last_instance = vectorizer\n","\n","        # Transform sparse matrix into Pandas DataFrame\n","        X_train_tfidf = pd.DataFrame.sparse.from_spmatrix(X_train_tfidf, columns=vectorizer.get_feature_names_out())\n","        X_test_tfidf = pd.DataFrame.sparse.from_spmatrix(X_test_tfidf, columns=vectorizer.get_feature_names_out())\n","\n","        return X_train_tfidf, X_test_tfidf\n","\n","\n","    def get_embeddings(self):\n","        # Convert multiple-column data into single-text data first\n","        X_train_raw, X_test_raw = self.get_raw()\n","\n","        # Apply OpenAI embeddings\n","        embeddings = OpenAIEmbeddings()\n","        X_train_embeddings = np.array(embeddings.embed_documents(X_train_raw))\n","        X_test_embeddings = np.array(embeddings.embed_documents(X_test_raw))\n","        self.last_instance = embeddings\n","\n","        # Convert to Pandas DataFrames\n","        X_train_embeddings = pd.DataFrame(X_train_embeddings, columns=[f\"Dim {x}\" for x in list(range(X_train_embeddings.shape[1]))])\n","        X_test_embeddings = pd.DataFrame(X_test_embeddings, columns=[f\"Dim {x}\" for x in list(range(X_test_embeddings.shape[1]))])\n","\n","        return X_train_embeddings, X_test_embeddings\n","\n","\n","    def get_felix(self, discrete_features=False, gpt4=False, callback=None, verbose=True):\n","        # Use GPT-4, if requested, or GPT-3.5-Turbo (4K context variant by default and 16K context variant as fallback if requests exceed context window)\n","        llm_name = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo\"\n","        llm_fallback = \"gpt-4-1106-preview\" if gpt4 else \"gpt-3.5-turbo-16k\"\n","\n","        # Transform the data using FELIX\n","        self.last_instance = FELIX(context=self.dataset.context_train, llm_name=llm_name, llm_fallback=llm_fallback, discrete_features=discrete_features, zero_shot=self.dataset.train_zero_shot, callback=callback, verbose=verbose)\n","        X_train_felix = self.last_instance.fit_transform(self.dataset.X_train, self.dataset.y_train)\n","        self.last_instance.context = self.dataset.context_test\n","        X_test_felix = self.last_instance.transform(self.dataset.X_test)\n","\n","        return X_train_felix, X_test_felix\n","\n","\n","    def one_hot_encode(self, X_train, X_test):\n","        # Concatenate the data\n","        X_train_test = pd.concat([X_train, X_test], axis=0)\n","\n","        # Perform one-hot encoding\n","        X_train_test_onehot = pd.get_dummies(X_train_test)\n","\n","        # Split the data back into train and test\n","        X_train_onehot = X_train_test_onehot.head(len(X_train))\n","        X_test_onehot = X_train_test_onehot.tail(len(X_test))\n","\n","        return X_train_onehot, X_test_onehot\n","\n","\n","    def impute_missing_values(self, X_train, X_test, col_drop_thres=0.1, n_neighbors=5):\n","        def impute(df):\n","            # Remove all columns that have more than 'col_drop_thres' missing values\n","            missing_percentage = df.isnull().mean()                                         # Calculate the percentage of missing values for each column\n","            cols_to_drop = missing_percentage[missing_percentage > col_drop_thres].index    # Identify columns that have more than 'col_drop_thres' % missing values\n","            df = df.drop(columns=cols_to_drop)                                              # Drop the columns from the DataFrame\n","\n","            # Estimate missing values with k-Nearest-Neighbors imputations\n","            imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n","            imputer.set_output(transform=\"pandas\")\n","            df = imputer.fit_transform(df)\n","\n","    \t    # Round all values to integers\n","            df = df.round().astype(int)\n","\n","            return df\n","\n","        return impute(X_train), impute(X_test)\n","\n","\n","    def get_model_instance(self):\n","        # Return the last trained model instance\n","        return self.last_instance"]},{"cell_type":"markdown","metadata":{"id":"_xxjjKOXG9ss"},"source":["## Run Tracking Functionality"]},{"cell_type":"markdown","source":["Set your Weights & Biases project in lines 21 and 22 of the following cell:"],"metadata":{"id":"u_5g7rgIuIOc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFkm0OU5HB2W"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","import pandas as pd\n","from datetime import datetime\n","import time\n","\n","\n","class Run:\n","\n","    def __init__(self, data_representation, classifier, dataset, seed=0):\n","        self.data_representation = [data_representation] if isinstance(data_representation, str) else data_representation\n","        self.classifier = [classifier] if isinstance(classifier, str) else classifier\n","        self.dataset = dataset\n","        self.seed = seed\n","\n","        self.data_transformer = DataTransformer(dataset=d)\n","        self.cache = {}\n","\n","        # Set Weights & Biases project reference for logging\n","        self.wandb_entity = \"...\"\n","        self.wandb_project = \"...\"\n","\n","\n","    def run(self):\n","        for dr in self.data_representation:\n","            for cls in self.classifier:\n","                self.single_run(dr, cls, self.dataset)\n","\n","\n","    def single_run(self, data_representation, classifier, dataset):\n","        print(f\"Starting new run '{data_representation} {classifier}' on '{dataset.id}' ...\")\n","\n","        # Initiate logging of a new run\n","        self.init_logging(data_representation, classifier, dataset)\n","\n","        # Prepare data representation\n","        if data_representation in self.cache:\n","            print(f\"Loading cached data transformation '{data_representation}' ...\")\n","            X_train_transformed, X_test_transformed, transformer_instance, callback, time_transformation = self.cache[data_representation]\n","        else:\n","            print(f\"Transforming data using '{data_representation}' ...\")\n","            start_time = time.time()\n","            X_train_transformed, X_test_transformed, transformer_instance, callback = self.transform_data(data_representation)\n","            time_transformation = time.time() - start_time\n","            self.cache[data_representation] = (X_train_transformed, X_test_transformed, transformer_instance, callback, time_transformation)\n","\n","        print(\"X_train.shape =\", X_train_transformed.shape)\n","        print(\"X_test.shape = \", X_test_transformed.shape)\n","\n","        # Keep copies of the transformed data (before one-hot encoding and imputation) for logging\n","        X_train_transformed_original = X_train_transformed\n","        X_test_transformed_original = X_test_transformed\n","\n","        # One-hot encode the data if is categorical\n","        if data_representation in [\"FELIX GPT-3.5 (Categorical)\", \"FELIX GPT-4 (Categorical)\"]:\n","            print(\"Creating one-hot encoding of categorical data ...\")\n","            X_train_transformed, X_test_transformed = self.data_transformer.one_hot_encode(X_train_transformed, X_test_transformed)\n","\n","        # Impute missing values if data is numerical\n","        elif data_representation in [\"FELIX GPT-3.5 (Numerical)\", \"FELIX GPT-4 (Numerical)\"]:\n","            print(\"Imputing missing values in numerical data ...\")\n","            X_train_transformed, X_test_transformed = self.data_transformer.impute_missing_values(X_train_transformed, X_test_transformed)\n","\n","        # Train the classifier on the training dataset and predict class labels on the test dataset\n","        start_time = time.time()\n","        print(f\"Training '{classifier}' on '{dataset.id}' training set ...\")\n","        self.classifier_instance = self.get_classifier(classifier)\n","\n","        # If data was classified via ZeroShotGPTClassifier, extract its callback to log API cost and consumption\n","        if isinstance(self.classifier_instance, ZeroShotGPTClassifier):\n","            callback = self.classifier_instance.callback\n","\n","        self.classifier_instance.fit(X_train_transformed, dataset.y_train)\n","        print(f\"Predicting class labels of '{dataset.id}' test set with trained '{classifier}' ...\")\n","        self.y_pred = self.classifier_instance.predict(X_test_transformed)\n","        time_classification = time.time() - start_time\n","\n","        # For ZeroShotGPT classifiers, treat missing predictions as incorrect answers\n","        if isinstance(self.classifier_instance, ZeroShotGPTClassifier):\n","            # Find the unique ground truth labels\n","            unique_labels = set(self.dataset.y_test)\n","\n","            # Treat missing predictions as incorrect answers by setting them to the incorrect ground truth class\n","            counter = 0\n","            for i in range(len(self.y_pred)):\n","                if self.y_pred[i] not in unique_labels:\n","                    # Find the incorrect label (the one not equal to the ground truth) and assign it as the prediction\n","                    self.y_pred[i] = (unique_labels - {self.dataset.y_test.to_list()[i]}).pop()\n","                    counter += 1\n","\n","            if counter > 0:\n","                print(f\"Treating {counter} missing values as incorrect answers.\")\n","\n","        # Log the final results and finish the run\n","        print(\"Finishing run and logging the results ...\")\n","        self.log(self.y_pred, X_train_transformed_original, X_test_transformed_original, self.classifier_instance, transformer_instance, callback, time_transformation, time_classification)\n","        self.finish()\n","\n","\n","    def transform_data(self, data_representation):\n","        if data_representation == \"TF-IDF\":\n","            callback = None\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_tfidf()\n","        elif data_representation == \"Embeddings\":\n","            callback = None\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_embeddings()\n","        elif data_representation == \"Raw Text\":\n","            callback = None\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_raw()\n","        elif data_representation == \"FELIX GPT-3.5 (Numerical)\":\n","            callback = CustomFELIXCallback()\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(discrete_features=False, gpt4=False, callback=callback, verbose=True)\n","        elif data_representation == \"FELIX GPT-3.5 (Categorical)\":\n","            callback = CustomFELIXCallback()\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(discrete_features=True, gpt4=False, callback=callback, verbose=True)\n","        elif data_representation == \"FELIX GPT-4 (Numerical)\":\n","            callback = CustomFELIXCallback()\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(discrete_features=False, gpt4=True, callback=callback, verbose=True)\n","        elif data_representation == \"FELIX GPT-4 (Categorical)\":\n","            callback = CustomFELIXCallback()\n","            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(discrete_features=True, gpt4=True, callback=callback, verbose=True)\n","        else:\n","            raise ValueError(f\"Invalid value '{data_representation}' for data representation. Must be one of ['TF-IDF', 'Embeddings', 'Raw Text', 'FELIX GPT-3.5 (Numerical)', 'FELIX GPT-3.5 (Categorical)', 'FELIX GPT-4 (Numerical)', 'FELIX GPT-4 (Categorical)'].\")\n","\n","        transformer_instance = self.data_transformer.get_model_instance()\n","\n","        return X_train_transformed, X_test_transformed, transformer_instance, callback\n","\n","\n","    def get_classifier(self, classifier):\n","        if classifier == \"RandomForest\":\n","            return RandomForestClassifier(random_state=self.seed)\n","        elif classifier == \"LogisticRegression\":\n","            return LogisticRegression(max_iter=10000, random_state=seed)\n","        elif classifier == \"GPT-3.5\":\n","            return ZeroShotGPTClassifier(llm_name=\"gpt-3.5-turbo\", callback=ZeroShotGPTCallback(), verbose=True)\n","        elif classifier == \"GPT-4\":\n","            return ZeroShotGPTClassifier(llm_name=\"gpt-4-1106-preview\", callback=ZeroShotGPTCallback(), verbose=True)\n","        elif classifier == \"RoBERTA-Base 1 Epoch\":\n","            return FineTunedClassificationLLM(model_name=\"FacebookAI/roberta-base\", epochs=1, seed=self.seed)\n","        elif classifier == \"RoBERTA-Base 10 Epochs\":\n","            return FineTunedClassificationLLM(model_name=\"FacebookAI/roberta-base\", epochs=10, seed=self.seed)\n","        elif classifier == \"RoBERTA-Base 50 Epochs\":\n","            return FineTunedClassificationLLM(model_name=\"FacebookAI/roberta-base\", epochs=50, seed=self.seed)\n","        elif classifier == \"RoBERTA-Base 100 Epochs\":\n","            return FineTunedClassificationLLM(model_name=\"FacebookAI/roberta-base\", epochs=100, seed=self.seed)\n","\n","        raise ValueError(f\"Invalid value '{classifier}' for classifier. Must be one of ['RandomForest', 'LogisticRegression', 'GPT-3.5', 'GPT-4'].\")\n","\n","\n","    def init_logging(self, data_representation, classifier, dataset):\n","        # Start a new Weights & Biases run to track results\n","        wandb.init(\n","            entity=self.wandb_entity,\n","            project=self.wandb_project,\n","            name=f\"{data_representation} {classifier}\",\n","            config={\n","                \"dataset\": dataset.id,\n","                \"dataset short name\": dataset.short_name,\n","                \"data_representation\": data_representation,\n","                \"model\": classifier,\n","                \"context_description\": dataset.context_train,\n","                \"context_description_test\": dataset.context_test,\n","                \"n_train\": dataset.X_train.shape[0],\n","                \"n_test\": dataset.X_test.shape[0],\n","                \"seed\": self.seed,\n","                \"pos_class\": dataset.pos_class,\n","                \"train_zero_shot\": dataset.train_zero_shot\n","            }\n","        )\n","\n","\n","    def log(self, y_pred, X_train_transformed, X_test_transformed, classifier=None, transformer=None, callback=None, time_transformation=-1.0, time_classification=-1.0):\n","        # Log classification performance scores\n","        results = self.calculate_scores(self.dataset.y_test, y_pred, pos_class=self.dataset.pos_class)\n","\n","        # Store the run duration\n","        results[\"Time Transformation\"] = time_transformation\n","        results[\"Time Classification\"] = time_classification\n","        results[\"Time Total\"] = time_transformation + time_classification\n","\n","        # Calculate API costs\n","        if callback and isinstance(callback, FELIXCallback):\n","            generation_cost = sum([x[\"Total Cost\"] for x in callback.generation_log])\n","            scoring_cost = sum([x[\"Total Cost\"] for x in callback.scoring_log])\n","            total_cost = generation_cost + scoring_cost\n","            results[\"Generation Cost\"] = generation_cost\n","            results[\"Scoring Cost\"] = scoring_cost\n","            results[\"Total Cost\"] = total_cost\n","        elif callback and isinstance(callback, ZeroShotGPTCallback):\n","            results[\"Total Cost\"] = sum([x[\"Total Cost\"] for x in callback.consumption_log])\n","\n","        # Save a detailed log of all LLM requests\n","        if callback and isinstance(callback, FELIXCallback):\n","            results[\"Generation Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.generation_log))\n","            results[\"Scoring Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.scoring_log))\n","        elif callback and isinstance(callback, ZeroShotGPTCallback):\n","            results[\"Prompt Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.consumption_log))\n","\n","        # Store feature insights from classifier models\n","        if classifier:\n","            # If features have been learned by FELIX, get its features descriptions\n","            if transformer and isinstance(transformer, FELIX):\n","                df_features = transformer.get_features_as_dataframe().rename(columns={\"Name\": \"Feature Name\"})\n","                if \"Possible Values\" in df_features.columns:\n","                    df_features[\"Possible Values\"] = df_features[\"Possible Values\"].apply(lambda x: str(x).strip(\"[]\").replace(\", \", \"\\n\"))\n","            else:\n","                df_features = None\n","\n","            if isinstance(classifier, RandomForestClassifier):\n","                df_importances = pd.DataFrame({\"Feature Name\": classifier.feature_names_in_, \"Importance\": classifier.feature_importances_}).sort_values(by=\"Importance\", ascending=False)  # Get feature importance scores from the Random Forest classifier\n","                if transformer and isinstance(transformer, FELIX) and transformer.discrete_features:\n","                    df_importances[[\"Feature Name\", \"Value\"]] = df_importances[\"Feature Name\"].str.rsplit(\"_\", n=1, expand=True)                                                            # Separate feature names and values from the one-hot encoding\n","                if isinstance(df_features, pd.DataFrame):\n","                    df_importances = df_importances.set_index(\"Feature Name\").join(df_features.set_index(\"Feature Name\"), how=\"left\").reset_index()                                         # Join the Random Forest feature importances with FELIX's feature descriptions\n","                results[\"Feature Importance\"] = df_importances\n","            elif isinstance(classifier, LogisticRegression):\n","                df_coefficients = pd.DataFrame({\"Feature Name\": classifier.feature_names_in_, \"Coefficient\": classifier.coef_[0]}).sort_values(by=\"Coefficient\", ascending=False)           # Get coefficients from the Logistic Regression classifier\n","                if transformer and isinstance(transformer, FELIX) and transformer.discrete_features:\n","                    df_coefficients[[\"Feature Name\", \"Value\"]] = df_coefficients[\"Feature Name\"].str.rsplit(\"_\", n=1, expand=True)                                                            # Separate feature names and values from the one-hot encoding\n","                if isinstance(df_features, pd.DataFrame):\n","                    df_coefficients = df_coefficients.set_index(\"Feature Name\").join(df_features.set_index(\"Feature Name\"), how=\"left\").reset_index()                                       # Join the Logistic Regression coefficients with FELIX's feature descriptions\n","                results[\"Coefficients\"] = df_coefficients\n","\n","        # Store the trained FELIX instance, if available\n","        if transformer:\n","            if isinstance(transformer, FELIX):\n","                filename = f\"FELIX-{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}.json\"\n","                transformer.save_instance(filename)\n","                wandb.save(filename)\n","\n","        wandb.log(results)\n","\n","        # Log training and test data\n","        try:\n","            wandb.log({\n","                \"Training Data\": wandb.Table(dataframe=pd.concat([pd.Series(self.dataset.y_train, name=\"Ground Truth\").reset_index(drop=True), self.dataset.X_train.reset_index(drop=True), X_train_transformed.reset_index(drop=True)], axis=1)),\n","                \"Test Data\": wandb.Table(dataframe=pd.concat([pd.Series(self.dataset.y_test, name=\"Ground Truth\").reset_index(drop=True), pd.Series(y_pred, name=\"Prediction\").reset_index(drop=True), self.dataset.X_test.reset_index(drop=True), X_test_transformed.reset_index(drop=True)], axis=1))\n","            })\n","        except TypeError as e:\n","            # Some formats of data seem to cause an internal TypeError within Weights & Biases that I could not solve yet\n","            print(e)\n","\n","        return results\n","\n","\n","    def calculate_scores(self, y_true, y_pred, pos_class):\n","        return {\n","            \"Accuracy\": accuracy_score(y_true, y_pred),\n","            \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n","            \"F1 Score (Macro)\": f1_score(y_true, y_pred, average=\"macro\"),\n","            \"F1 Score (Positive Class)\": f1_score(y_true, y_pred, pos_label=pos_class),\n","            \"Precision (Macro)\": precision_score(y_true, y_pred, average=\"macro\"),\n","            \"Precision (Positive Class)\": precision_score(y_true, y_pred, pos_label=pos_class),\n","            \"Recall (Macro)\": recall_score(y_true, y_pred, average=\"macro\"),\n","            \"Recall (Positive Class)\": recall_score(y_true, y_pred, pos_label=pos_class)\n","        }\n","\n","\n","    def finish(self):\n","        wandb.finish()\n","\n","\n","\n","class CustomFELIXCallback(FELIXCallback):\n","\n","    def __init__(self):\n","        self.generation_log = []\n","        self.scoring_log = []\n","        self.error_log = []\n","\n","\n","    def features_generated_for_pair(self, llm, example_a: str, example_b: str, label_a: str, label_b: str, system_message: str, prompt_message: str, llm_output: str, features: NumericalFeatureSet | CategoricalFeatureSet, total_cost, total_tokens, prompt_tokens, completion_tokens):\n","        self.generation_log.append({\n","            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n","            \"LLM\": llm.model_name,\n","            \"Temperature\": llm.temperature,\n","            \"Example A\": example_a,\n","            \"Label A\": label_a,\n","            \"Example B\": example_b,\n","            \"Label B\": label_b,\n","            \"System Message\": system_message,\n","            \"Prompt Message\": prompt_message,\n","            \"LLM Output\": llm_output,\n","            \"Features\": str(features.features),\n","            \"Total Cost\": total_cost,\n","            \"Total Tokens\": total_tokens,\n","            \"Prompt Tokens\": prompt_tokens,\n","            \"Completion Tokens\": completion_tokens\n","        })\n","\n","\n","    def example_transformed(self, llm, example: str, feature_set: NumericalFeatureSet | CategoricalFeatureSet, system_message: str, prompt_message: str, llm_output: str, scores, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n","        self.scoring_log.append({\n","            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n","            \"LLM\": llm.model_name,\n","            \"Temperature\": llm.temperature,\n","            \"Example\": example,\n","            \"Features\": str(feature_set.features),\n","            \"System Message\": system_message,\n","            \"Prompt Message\": prompt_message,\n","            \"LLM Output\": llm_output,\n","            \"Scores\": str(scores),\n","            \"Total Cost\": total_cost,\n","            \"Total Tokens\": total_tokens,\n","            \"Prompt Tokens\": prompt_tokens,\n","            \"Completion Tokens\": completion_tokens\n","        })\n","\n","\n","    def error_encountered(self, llm, system_message, prompt_message, llm_output, error):\n","        self.error_log.append({\n","            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n","            \"LLM\": llm,\n","            \"System Message\": system_message,\n","            \"Prompt Message\": prompt_message,\n","            \"LLM Output\": llm_output,\n","            \"Error\": error\n","        })"]},{"cell_type":"markdown","metadata":{"id":"FqArEL7qW7zV"},"source":["# Experiment 1: Overall Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqJZcVqnTOYl"},"outputs":[],"source":["d = dataset_list[\"reviews-amazon\"]\n","d"]},{"cell_type":"markdown","metadata":{"id":"aRTHCj0pm4A0"},"source":["## TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dVaK8droJlcE"},"outputs":[],"source":["run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"2MiK2EYF2EXw"},"source":["## Text Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tP5XpR6Jo0z"},"outputs":[],"source":["run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"QAb2W0GY3pL9"},"source":["## GPT-3.5 Zero-Shot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mY9aMyhzJzV-"},"outputs":[],"source":["run = Run(\"Raw Text\", \"GPT-3.5\", d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"lbMrSGdqt3-8"},"source":["## GPT-4 Zero-Shot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kw07r5BHJyyH"},"outputs":[],"source":["run = Run(\"Raw Text\", \"GPT-4\", d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"tfWTZa8TJvZg"},"source":["## FELIX GPT-3.5 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jC1FuoGwJvZy"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"gJ2d6PgwJvZy"},"source":["## FELIX GPT-3.5 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dpfaoaXsJvZy"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"xiStGdKtJvZz"},"source":["## FELIX GPT-4 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oTOypqk9JvZz"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"0S9dIUpqJvZz"},"source":["## FELIX GPT-4 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GqpNApcTJvZz"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","source":["## Fine-Tuned LLM"],"metadata":{"id":"zceU5zcjKOTU"}},{"cell_type":"code","source":["run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d, seed + i)\n","run.run()"],"metadata":{"id":"4AST-ZiBMW9o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EyDb-RF5RXJ"},"source":["# Experiment 2: Sample Efficiency"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-CmyOa_tHl-v"},"outputs":[],"source":["d = dataset_list[\"sentiment\"]\n","n_train = 10\n","\n","d = Dataset(\n","    id=d.id,\n","    short_name=d.short_name,\n","    X_train=d.X_train.head(n_train),\n","    X_test=d.X_test,\n","    y_train=d.y_train.head(n_train),\n","    y_test=d.y_test,\n","    pos_class=d.pos_class,\n","    context_train=d.context_train,\n","    context_test=d.context_test,\n","    train_zero_shot=False\n",")\n","\n","d"]},{"cell_type":"markdown","metadata":{"id":"wjMrXOviI7Pe"},"source":["## TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nZAZ5WP3I9HU"},"outputs":[],"source":["run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"XbRgFopLJp3n"},"source":["## Text Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Sl_gHTjsHvoB"},"outputs":[],"source":["run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"vsCcfPIaJze9"},"source":["## FELIX GPT-3.5 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R1wKRIdJKEN7"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"dYxoWRAnJ69Y"},"source":["## FELIX GPT-3.5 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NADeCdscKLy1"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"yQsICM76J8_i"},"source":["## FELIX GPT-4 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VCh8Lh2IKOYA"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"v30AJkd0J-Kk"},"source":["## FELIX GPT-4 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddlZUt3-JHAW"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","source":["## Fine-Tuned LLM"],"metadata":{"id":"ftecAjQuOmIG"}},{"cell_type":"code","source":["run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d_sample, seed + i)\n","run.run()"],"metadata":{"id":"OKMBVZadOoEC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rax2gybu5Y2M"},"source":["# Experiment 3: Domain Adaptation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ils-5hqp5cmB"},"outputs":[],"source":["train_name = \"reviews-amazon\"\n","test_name = \"reviews-yelp\"\n","\n","d_train = dataset_list[train_name]\n","d_test = dataset_list[test_name]\n","\n","if d_train.pos_class != d_test.pos_class:\n","    print(\"Warning: Train and test datasets have different classes. Do you still want to continue?\\n\")\n","\n","d = Dataset(\n","    id=f\"{d_train.id}>{d_test.id}\",\n","    short_name=f\"{d_train.short_name}>{d_test.short_name}\",\n","    X_train=d_train.X_train,\n","    X_test=d_test.X_test,\n","    y_train=d_train.y_train,\n","    y_test=d_test.y_test,\n","    pos_class=d_train.pos_class,\n","    context_train=d_train.context_train,\n","    context_test=d_test.context_test,\n","    train_zero_shot=False\n",")\n","\n","d"]},{"cell_type":"markdown","metadata":{"id":"wmpScVWCS7Vh"},"source":["## TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"P6GqeGO2S1sI"},"outputs":[],"source":["run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"PUEEZW0LTVmc"},"source":["## Text Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zw9AmXp7TVmk"},"outputs":[],"source":["run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"HKCfr7hrTVml"},"source":["## FELIX GPT-3.5 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KDXgj0rpTVml"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"1lxVRAcdTVml"},"source":["## FELIX GPT-3.5 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I15YVO_wTVml"},"outputs":[],"source":["run = Run(\"FELIX GPT-3.5 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"4fXd0nhzTVml"},"source":["## FELIX GPT-4 (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qTBVyVnOTVmm"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","metadata":{"id":"BtC25X2ATVmm"},"source":["## FELIX GPT-4 (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bxBPrqMTVmm"},"outputs":[],"source":["run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n","run.run()"]},{"cell_type":"markdown","source":["## Fine-Tuned LLM"],"metadata":{"id":"fO6m5GNiPCi2"}},{"cell_type":"code","source":["run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d, seed + i)\n","run.run()"],"metadata":{"id":"AR_6SpsxPEQE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SLghYGJGVrF"},"source":["# Experiment 5: Internal Validity"]},{"cell_type":"markdown","metadata":{"id":"ChrboR85HquD"},"source":["## Experiment 5.1: Clustering Validity"]},{"cell_type":"markdown","metadata":{"id":"7FLyokbtBx3E"},"source":["### Learn Numeric Feature Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNSJksX_cmG3"},"outputs":[],"source":["results_log = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQzzM8mFkUxM"},"outputs":[],"source":["def log_results(dataset_id, method, felix, features, f1_rf, f1_lr, probs_lr, comment=\"\"):\n","    results_log.append({\n","        \"dataset\": dataset_id,\n","        \"method\": method,\n","        \"felix_variant\": f\"FELIX GPT-{'3.5' if '3.5' in felix.llm_scoring else '4'} {'Categorical' if felix.discrete_features else 'Numerical'}\",\n","        \"n_features\": len(features),\n","        \"feature_set\": features,\n","        \"f1_rf\": f1_rf,\n","        \"f1_lr\": f1_lr,\n","        \"probs_lr\": probs_lr,\n","        \"comment\": comment\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3OdZZACBQCt"},"outputs":[],"source":["# Select a dataset\n","d = dataset_list[\"sentiment\"]\n","d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RV7c_fNhB2uu"},"outputs":[],"source":["# Configure FELIX\n","felix = FELIX(\n","    context=d.context_train,\n","    temperature_scoring=0.0,\n","    llm_scoring=\"gpt-3.5-turbo-16k\",\n","    discrete_features=False,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cIvIemyj7zS"},"outputs":[],"source":["n_generation = 30\n","\n","# Fit FELIX to the dataset to learn features\n","felix._full_feature_set = felix.generate_features(d.X_train.head(n_generation), d.y_train.head(n_generation))\n","felix._full_feature_set = felix._ensure_unique_feature_names(felix._full_feature_set)\n","felix._features = felix._full_feature_set"]},{"cell_type":"markdown","metadata":{"id":"jCKXJUJglvu6"},"source":["Save the learned features for later use:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTM_7yhy-GKs"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","\n","# Serializing json\n","json_object = felix._features.json(indent=4)\n","\n","# Store the results\n","filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Features {d.short_name}.json\"\n","with open(filename, \"w\") as f:\n","    f.write(json_object)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STGoRxof-MBR"},"outputs":[],"source":["from google.colab import files\n","import json\n","\n","# Upload the features JSON\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Parse the results\n","flat_list = json.loads(uploaded[filename])[\"features\"]\n","if felix.discrete_features:\n","    felix._features = CategoricalFeatureSet(features=flat_list)\n","else:\n","    felix._features = NumericalFeatureSet(features=flat_list)\n","felix._full_feature_set = felix._features\n","len(felix._features.features)"]},{"cell_type":"markdown","metadata":{"id":"d90sEUGRya5d"},"source":["### Score All Learned Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHk9EdaDDTJq"},"outputs":[],"source":["df_scores = felix.transform(d.X_test)\n","df_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfw3bEF2-j3f"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","\n","# Save the scores data as a CSV\n","filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Scores {d.short_name}.csv\"\n","df_scores.to_csv(filename, index=False)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMH0NElCN8tb"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","\n","# Upload the scores CSV\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Load the scores\n","df_scores = pd.read_csv(filename)\n","df_scores"]},{"cell_type":"markdown","metadata":{"id":"OzTgw82m6KJp"},"source":["### Handle Missing Values (Numerical)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0R8v9dlCxa4"},"outputs":[],"source":["col_drop_thres = 0.1\n","n_neighbors = 5\n","\n","# Remove all columns that have more than 'col_drop_thres' missing values\n","missing_percentage = df_scores.isnull().mean()                                  # Calculate the percentage of missing values for each column\n","cols_to_drop = missing_percentage[missing_percentage > col_drop_thres].index    # Identify columns that have more than 'col_drop_thres' % missing values\n","df_scores = df_scores.drop(columns=cols_to_drop)                                # Drop the columns from the DataFrame\n","\n","# Estimate missing values with k-Nearest-Neighbors imputations\n","imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n","imputer.set_output(transform=\"pandas\")\n","df_scores = imputer.fit_transform(df_scores)\n","\n","print(\"Columns removed:\", cols_to_drop)\n","print(\"Columns with missing features remaining:\", df_scores.isnull().any().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nw8YGtTNDx53"},"outputs":[],"source":["# Remove features from FELIX that could not be scored reliably (i.e., more than 'col_drop_thres' missing values)\n","felix._features.features = [f for f in felix._features.features if f.name not in cols_to_drop]\n","\n","print(len(felix._features.features))\n","print(df_scores.shape)"]},{"cell_type":"markdown","metadata":{"id":"OYp8UxbpDtv4"},"source":["### Handle Missing Values (Categorical)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vl10p0MK6JnC"},"outputs":[],"source":["nan_columns = df_scores.isna().all()[df_scores.isna().all() == True].index.to_list()\n","\n","print(nan_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kicLl5cX6xmS"},"outputs":[],"source":["# Remove features from FELIX that could not be scored (i.e., all rows have NaN values)\n","felix._features.features = [f for f in felix._features.features if f.name not in nan_columns]\n","\n","# Remove respective columns from the scores dataset\n","df_scores = df_scores.drop(columns=nan_columns)\n","\n","print(len(felix._features.features))\n","print(df_scores.shape)"]},{"cell_type":"markdown","metadata":{"id":"qT985Mpuk533"},"source":["### Create Feature Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78mM1q0Gr-0f"},"outputs":[],"source":["# Create text embeddings for each feature\n","felix._feature_embeddings = felix._create_feature_embeddings(felix._features)"]},{"cell_type":"markdown","metadata":{"id":"iujEVCf2ylaY"},"source":["### Prepare Feature Selection Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xg-zyQzG9fJR"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","def encode_data(X_train, X_test):\n","    # Drop columns that have no values\n","    nan_columns_train = X_train.isna().all()[X_train.isna().all() == True].index.to_list()\n","    nan_columns_test = X_test.isna().all()[X_test.isna().all() == True].index.to_list()\n","    # nan_columns = nan_columns_train + nan_columns_test\n","    nan_columns = [x for x in nan_columns_train if x in nan_columns_test]\n","    X_train = X_train.drop(columns=nan_columns)\n","    X_test = X_test.drop(columns=nan_columns)\n","\n","    # Identify categorical columns (assuming the same columns in train and test)\n","    categorical_columns = X_train.select_dtypes(include=[object]).columns\n","    non_categorical_columns = X_train.select_dtypes(exclude=[object]).columns\n","\n","    # Encode values based on column type\n","    if len(categorical_columns) > 0 and len(non_categorical_columns) == 0:\n","        data_transformer = DataTransformer(dataset=None)\n","        return data_transformer.one_hot_encode(X_train, X_test)\n","    elif len(non_categorical_columns) > 0 and len(categorical_columns) == 0:\n","        return X_train, X_test\n","    else:\n","        print(\"Categorical columns =\", categorical_columns)\n","        print(\"Numerical columns =\", non_categorical_columns)\n","        print(\"NaN columns training =\", nan_columns_train)\n","        print(\"NaN columns test =\", nan_columns_test)\n","        print(\"NaN columns =\", nan_columns)\n","        raise ValueError(\"Cannot handle mixed dataset of categorical and numerical columns.\")\n","\n","\n","def evaluate_feature_set(feature_set, X, y, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True):\n","    # Define classifier models\n","    if lr_probs or lr_f1:\n","        lr_model = LogisticRegression(max_iter=10000, random_state=random_state)\n","    if rf_f1:\n","        rf_model = RandomForestClassifier(random_state=random_state)\n","\n","    # Create lists to store the scores from each cross-validation fold\n","    avg_probs_lr = []\n","    avg_scores_lr = []\n","    avg_scores_rf = []\n","\n","    # Evaluate performance using k-fold cross validation\n","    kf = KFold(n_splits=cv_splits)\n","    for train_index, test_index in kf.split(X):\n","        # Create train-test split\n","        X_train = X.iloc[train_index][feature_set]\n","        y_train = y.iloc[train_index]\n","        X_test = X.iloc[test_index][feature_set]\n","        y_test = y.iloc[test_index]\n","\n","        # Convert columns that contain categorical data into a one-hot representation\n","        X_train, X_test = encode_data(X_train, X_test)\n","\n","        # Train the models with the feature set\n","        if lr_probs or lr_f1:\n","            lr_model.fit(X_train, y_train)\n","        if rf_f1:\n","            rf_model.fit(X_train, y_train)\n","\n","        # Evaluate the trained models on the test set\n","        if lr_probs:\n","            probs_lr = lr_model.predict_proba(X_test)\n","            avg_prob = np.mean([p[lr_model.classes_.tolist().index(c)] for p, c in zip(probs_lr, y_test)]) # Calculate the average predicted probability of belonging to the ground truth class\n","            avg_probs_lr.append(avg_prob)\n","        else:\n","            avg_probs_lr.append(0.0)\n","        if lr_f1:\n","            preds_lr = lr_model.predict(X_test)\n","            f1_lr = f1_score(y_test, preds_lr, average=\"macro\")\n","            avg_scores_lr.append(f1_lr)\n","        else:\n","            avg_scores_lr.append(0.0)\n","        if rf_f1:\n","            preds_rf = rf_model.predict(X_test)\n","            f1_rf = f1_score(y_test, preds_rf, average=\"macro\")\n","            avg_scores_rf.append(f1_rf)\n","        else:\n","            avg_scores_rf.append(0.0)\n","\n","    # Calculate and return average scores across all folds\n","    return np.mean(avg_probs_lr), np.mean(avg_scores_lr), np.mean(avg_scores_rf)\n","\n","\n","def forward_selection(X, y, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True):\n","    # Create lists to store the features selected (in order of forward selection) and corresponding scores\n","    feature_set = []\n","    scores = []\n","\n","    # Iteratively add the feature that leads to the best classification performance\n","    while len(feature_set) < X.shape[1]:\n","        # Keep track of the best performance so far\n","        best_feature = None\n","        best_score = -np.inf\n","        prob_lr = -np.inf\n","        score_lr = -np.inf\n","        score_rf = -np.inf\n","\n","        # Iterate over all features that are not yet in the feature set and find the one that leads to the best performance\n","        for feature in X.columns:\n","            if feature not in feature_set:\n","                # Add the current feature to the set\n","                features_to_use = feature_set + [feature]\n","\n","                # Evaluate the classification performance using this feature set\n","                lr_prob, lr_score, rf_score = evaluate_feature_set(features_to_use, X, y, cv_splits, random_state, lr_probs=lr_probs, lr_f1=lr_f1, rf_f1=rf_f1)\n","\n","                # Calculate the final decision criterion\n","                if lr_f1 and rf_f1:\n","                    score = np.mean([lr_score, rf_score])\n","                elif lr_f1:\n","                    score = lr_score\n","                elif rf_f1:\n","                    score = rf_score\n","                elif lr_probs:\n","                    score = lr_prob\n","\n","                # Check if this is the best score\n","                if score > best_score:\n","                    best_feature = feature\n","                    best_score = score\n","\n","                    # Also store the other scores as reference\n","                    prob_lr = lr_prob\n","                    score_lr = lr_score\n","                    score_rf = rf_score\n","\n","        # Add best feature to our feature set\n","        feature_set.append(best_feature)\n","        scores.append(best_score)\n","\n","        log_results(d.id, \"Supervised Forward Selection\", felix, feature_set, score_rf, score_lr, prob_lr, f\"Added feature {best_feature}\")\n","\n","        print(f\"n_features = {len(feature_set)}. Score = {best_score}. LR_prob = {prob_lr}. LR_F1 = {score_lr}. RF_F1 = {score_rf}. Added feature {best_feature}\")\n","\n","    return feature_set, scores"]},{"cell_type":"markdown","metadata":{"id":"DeZKGOG7n1QW"},"source":["### Supervised Forward Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8gWzPnlFIRg"},"outputs":[],"source":["features_forward, scores_forward = forward_selection(df_scores, d.y_test, cv_splits=5, random_state=seed, lr_probs=True, lr_f1=True, rf_f1=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR32mZMiAVtj"},"outputs":[],"source":["from google.colab import files\n","\n","df_forward = pd.DataFrame({\"Feature\": features_forward, \"F1 (LR-RF avg.)\": scores_forward})\n","\n","filename = \"2023_11_02 - FELIX Forward Selection Fake News.csv\"\n","df_forward.to_csv(filename, index=False)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yIMXSEgRlAz"},"outputs":[],"source":["# Upload the results\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Load the scores\n","df_forward = pd.read_csv(filename)\n","\n","features_forward = df_forward[\"Feature\"].tolist()\n","scores_forward = df_forward[\"F1 (LR-RF avg.)\"].tolist()"]},{"cell_type":"markdown","metadata":{"id":"54KJ85svoAB2"},"source":["### HDBSCAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rL0431PxLPlG"},"outputs":[],"source":["def run_hdbscan(feature_embeddings, df_scores, d, keep_noise=True, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0, verbose=False):\n","    # Cluster the features with HDBSCAN\n","    np.random.seed(0)\n","    hdbscan = HDBSCAN(min_cluster_size=2, allow_single_cluster=True, cluster_selection_method=cluster_selection_method, cluster_selection_epsilon=cluster_selection_epsilon, alpha=alpha)\n","    cluster_labels = hdbscan.fit_predict(feature_embeddings)\n","\n","    if keep_noise:\n","        # Assign all features identified as noise (i.e., not part of a cluster) to their own cluster\n","        next_label = np.max(cluster_labels) + 1\n","        for i in range(len(cluster_labels)):\n","            if cluster_labels[i] == -1:\n","                cluster_labels[i] = next_label\n","                next_label += 1\n","\n","        # Select the most representative feature from each cluster\n","        features_hdbscan = felix._select_representative_features(felix._features, felix._feature_embeddings, cluster_labels)\n","        if verbose:\n","            print(f\"HDBSCAN selected {len(features_hdbscan.features)} features (including noise)\")\n","    else:\n","        # If all features have been identified as noise, put them all in one cluster\n","        if (np.array(cluster_labels) == -1).all():\n","            cluster_labels = [0 for _ in cluster_labels]\n","            if verbose:\n","                print(\"All features identified as noise. Treating them as one cluster\")\n","\n","        # Prune the feature set to only those features that are not identified as noise\n","        pruned_feature_set = felix._features.copy(deep=True)\n","        pruned_feature_set.features = [f for f, l in zip(felix._features.features, cluster_labels) if l != -1]\n","        pruned_embeddings = np.array([e for e, l in zip(felix._feature_embeddings, cluster_labels) if l != -1])\n","        pruned_labels = [l for l in cluster_labels if l != -1]\n","\n","        # Select the most representative feature from each cluster (excluding noise)\n","        features_hdbscan = felix._select_representative_features(pruned_feature_set, pruned_embeddings, pruned_labels)\n","        if verbose:\n","            print(f\"HDBSCAN selected {len(features_hdbscan.features)} features (without noise)\")\n","\n","\n","    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_hdbscan.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n","    score_hdbscan = np.mean([lr_score, rf_score])\n","\n","    return features_hdbscan, rf_score, lr_score, lr_prob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9EpBU2ujs8k"},"outputs":[],"source":["features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=True, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0)\n","log_results(d.id, \"HDBSCAN (w/ noise)\", felix, [f.name for f in features_hdbscan.features], rf_score, lr_score, lr_prob, comment=\"keep_noise=True, cluster_selection_method='leaf', cluster_selection_epsilon=0.0, alpha=1.0\")\n","print(f\"F1 = {np.mean([rf_score, lr_score])}. n_features = {len(features_hdbscan.features)}\")\n","\n","features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=False, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0)\n","log_results(d.id, \"HDBSCAN (w/o noise)\", felix, [f.name for f in features_hdbscan.features], rf_score, lr_score, lr_prob, comment=\"keep_noise=False, cluster_selection_method='leaf', cluster_selection_epsilon=0.0, alpha=1.0\")\n","print(f\"F1 = {np.mean([rf_score, lr_score])}. n_features = {len(features_hdbscan.features)}\")"]},{"cell_type":"markdown","metadata":{"id":"fY1n8-8jlt1M"},"source":["Perform a grid search for the optimal HDBSCAN hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRzImoSFWvif"},"outputs":[],"source":["options_keep_noise = [True, False]\n","options_cluster_selection_method = [\"leaf\", \"eom\"]\n","options_cluster_selection_epsilon = [float(x) / 10 for x in range(0, 11, 2)]\n","options_alpha = [float(x) / 10 for x in range(5, 16, 1)]\n","\n","tuning_results = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vN0pslcQHi_"},"outputs":[],"source":["for kn in options_keep_noise:\n","    for csm in options_cluster_selection_method:\n","        for epsilon in options_cluster_selection_epsilon:\n","            for alpha in options_alpha:\n","                features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=kn, cluster_selection_method=csm, cluster_selection_epsilon=epsilon, alpha=alpha)\n","                r = {\n","                    \"Dataset\": d.id,\n","                    \"keep_noise\": kn,\n","                    \"cluster_selection_method\": csm,\n","                    \"epsilon\": epsilon,\n","                    \"alpha\": alpha,\n","                    \"F1 Average\": np.mean([rf_score, lr_score]),\n","                    \"F1 LR\": lr_score,\n","                    \"F1 RF\": rf_score,\n","                    \"n_features\": len(features_hdbscan.features)\n","                }\n","                tuning_results.append(r)\n","                print(r)"]},{"cell_type":"markdown","metadata":{"id":"fYTZq8GZmBem"},"source":["Download/upload the results of hyperparameter optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvlKh4FVhQ6l"},"outputs":[],"source":["df_tuning_results = pd.DataFrame(tuning_results)\n","df_tuning_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ouefc3K1xi-"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","\n","# Save the scores data as a CSV\n","filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - HDBSCAN Hyperparameter Optimization.csv\"\n","df_tuning_results.to_csv(filename, index=False)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VonexOk42ZG3"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","\n","# Upload the scores CSV\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Load the scores\n","df_tuning_results = pd.read_csv(filename)\n","df_tuning_results"]},{"cell_type":"markdown","metadata":{"id":"O0T3CKWEmLDM"},"source":["Plot the results of hyperparameter optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Wwp2SJen6uK"},"outputs":[],"source":["# Simplify the dataset IDs for visualization\n","df_tuning_results[\"Dataset\"] = df_tuning_results[\"Dataset\"].map({\n","    \"cardiffnlp/tweet_sentiment_multilingual\": \"Sentiment\",\n","    \"hate_speech18\": \"Hate Speech\",\n","    \"amazon_polarity\": \"Amazon\",\n","    \"GonzaloA/fake_news\": \"Fake News\",\n","    \"tum-nlp/IDMGSP\": \"Papers\"\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXXEsYVYo1EM"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.ticker as mtick\n","from google.colab import files\n","\n","# Set up the matplotlib figure\n","fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Unpack the axis for easy reference\n","(ax1, ax2) = axs\n","\n","# Plot boxplot for keep_noise\n","sns.boxplot(x='Dataset', y='F1 Average', hue='keep_noise', data=df_tuning_results, ax=ax1)\n","ax1.set_title(\"Performance for Different Values of 'keep_noise'\")\n","ax1.set_ylabel(\"F1 Score\")\n","ax1.set_xlabel(\"\")\n","ax1.set_ylim([0.3, 1.0])\n","ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n","\n","# Plotting boxplot for cluster_selection_method\n","sns.boxplot(x='Dataset', y='F1 Average', hue='cluster_selection_method', data=df_tuning_results, ax=ax2)\n","ax2.set_title(\"Performance for Different Values of 'cluster_selection_method'\")\n","ax2.set_ylabel(\"F1 Score\")\n","ax2.set_xlabel(\"\")\n","ax2.set_ylim([0.3, 1.0])\n","ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n","\n","# Add an overall title\n","fig.suptitle(\"HDBSCAN: Hyperparameter Tuning (Categorical Features)\")\n","\n","# Adjust the layout\n","plt.tight_layout()\n","\n","# Download the plot as PDF\n","filename = \"HDBSCAN Categorical Hyperparameter Optimization A.pdf\"\n","plt.savefig(filename)\n","files.download(filename)\n","\n","# Display the plots\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCz-HCJp-n8b"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Filter the dataframe for the optimal values of other hyperparameters\n","df_filtered = df_tuning_results[\n","    (df_tuning_results['keep_noise'] == False) &\n","    (df_tuning_results['cluster_selection_method'] == 'leaf')\n","]\n","\n","# Get unique datasets\n","unique_datasets = df_filtered['Dataset'].unique()\n","\n","# Prepare contour plots data with the filtered dataframe\n","contour_plots_data = []\n","for dataset in unique_datasets:\n","    dataset_df = df_filtered[df_filtered['Dataset'] == dataset]\n","    dataset_grouped = dataset_df.groupby(['alpha', 'epsilon'])['F1 Average'].mean().reset_index()\n","    pivot_table = dataset_grouped.pivot(index='alpha', columns='epsilon', values='F1 Average')\n","    contour_plots_data.append((dataset, pivot_table))\n","\n","# Calculate the average F1 score across all datasets\n","average_f1 = df_filtered.groupby(['alpha', 'epsilon'])['F1 Average'].mean().reset_index()\n","average_f1_pivot = average_f1.pivot(index='alpha', columns='epsilon', values='F1 Average')\n","contour_plots_data.append(('Average across Datasets', average_f1_pivot))\n","\n","# Determine global min and max F1 scores for consistent color scaling across all plots\n","f1_min = min(data.min().min() for _, data in contour_plots_data)\n","f1_max = 1.0\n","\n","# Create a new figure for the plots\n","fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n","\n","# Flatten the axes array for easy indexing\n","axes = axes.flatten()\n","\n","# Plot each dataset with the unified color scaling\n","for i, (dataset, data) in enumerate(contour_plots_data):\n","    sns.heatmap(data, ax=axes[i], cmap=\"YlGnBu_r\", cbar=False, linewidths=0, vmin=f1_min, vmax=f1_max)\n","    axes[i].invert_yaxis()\n","    axes[i].set_title(f'{dataset}')\n","    axes[i].set_ylabel('Alpha' if i % 3 == 0 else '')   # Only show the y axis label once for each row\n","    axes[i].set_xlabel('Epsilon' if i >= 3 else '')     # Only show the x axis label once for each column\n","    axes[i].set_yticks(axes[i].get_yticks(), axes[i].get_yticklabels(), rotation=0)\n","\n","# Place a color bar at the right of the plots\n","cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])\n","cbar = fig.colorbar(axes[0].collections[0], cax=cbar_ax)\n","cbar.set_label('F1 Score')\n","cbar.set_ticks(np.linspace(f1_min, f1_max, num=5))\n","cbar.ax.set_yticklabels([f'{tick:.0%}' for tick in cbar.get_ticks()])\n","\n","# Add an overall title\n","fig.suptitle(\"HDBSCAN: Hyperparameter Tuning (Numerical Features)\")\n","\n","# Adjust layout for better fit and to make room for the color bar\n","fig.subplots_adjust(right=0.85)\n","\n","# Download the plot as PDF\n","filename = \"HDBSCAN Numerical Hyperparameter Optimization B.pdf\"\n","plt.savefig(filename, bbox_inches='tight')\n","files.download(filename)\n","\n","# Display the plots\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyV9zhjOssfL"},"outputs":[],"source":["clusters_hdbscan = felix._cluster_features(felix._feature_embeddings)\n","features_hdbscan = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_hdbscan)\n","print(f\"HDBSCAN selected {len(features_hdbscan.features)} features\")\n","\n","lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_hdbscan.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n","score_hdbscan = np.mean([lr_score, rf_score])\n","print(f\"Score = {score_hdbscan}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"]},{"cell_type":"markdown","metadata":{"id":"PAORS-LFoCyN"},"source":["### K-means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk66JXllrOme"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","scores_kmeans = []\n","\n","for n_clusters in range(1, len(df_scores.columns)+1):\n","    kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\")\n","    kmeans.fit(felix._feature_embeddings)\n","    clusters_kmeans = kmeans.labels_\n","    features_kmeans = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_kmeans)\n","\n","    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_kmeans.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n","    score = np.mean([lr_score, rf_score])\n","    scores_kmeans.append(score)\n","\n","    log_results(d.id, \"K-means\", felix, [f.name for f in features_kmeans.features], rf_score, lr_score, lr_prob)\n","\n","    print(f\"n_features = {n_clusters}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"]},{"cell_type":"markdown","metadata":{"id":"ahsiWVgUoFnA"},"source":["### Hierarchical Agglomerative Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYr5ZpbXrOji"},"outputs":[],"source":["import numpy as np\n","from scipy.cluster.hierarchy import dendrogram, fcluster\n","from sklearn.cluster import AgglomerativeClustering\n","\n","# Fit agglomerative hierarchical clustering\n","agg_clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n","agg_clustering.fit(felix._feature_embeddings)\n","\n","# Create a linkage matrix from the AgglomerativeClustering output\n","num_samples = len(agg_clustering.labels_)\n","linkage_matrix = np.column_stack([\n","    agg_clustering.children_,\n","    agg_clustering.distances_,\n","    [num_samples + i for i in range(num_samples - 1)]\n","])\n","\n","scores_agg = []\n","\n","for n_clusters in range(1, len(df_scores.columns)+1):\n","    # Use fcluster to get the cluster labels based on a threshold for number of clusters\n","    threshold = linkage_matrix[:, 2][-(n_clusters - 2)]\n","    clusters_agg = fcluster(linkage_matrix, t=threshold, criterion=\"distance\")\n","\n","    features_agg = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_agg)\n","\n","    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_agg.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n","    score = np.mean([lr_score, rf_score])\n","    scores_agg.append(score)\n","\n","    log_results(d.id, \"Agglomerative Clustering\", felix, [f.name for f in features_agg.features], rf_score, lr_score, lr_prob)\n","\n","    print(f\"n_features = {n_clusters}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"]},{"cell_type":"markdown","metadata":{"id":"IREyoPaBoOTn"},"source":["### Random Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"KhaqU4azkAsk"},"source":["Run random feature selection 5 times for each dataset and average the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92bQY-e5VT13"},"outputs":[],"source":["import random\n","\n","for i in range(5):\n","    scores_random = []\n","\n","    # Create a random pertubation of the feature set\n","    features_shuffled = felix._features.copy(deep=True)\n","    random.shuffle(features_shuffled.features)\n","\n","    for n_features in range(1, len(df_scores.columns)+1):\n","        feature_set = features_shuffled.copy(deep=True)\n","        feature_set.features = features_shuffled.features[:n_features]\n","        feature_names = [f.name for f in features_shuffled.features[:n_features]]\n","\n","        lr_prob, lr_score, rf_score = evaluate_feature_set(feature_names, df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n","        score = np.mean([lr_score, rf_score])\n","        scores_random.append(score)\n","\n","        log_results(d.id, \"Random Feature Selection\", felix, feature_names, rf_score, lr_score, lr_prob)\n","\n","        print(f\"iteration = {i + 1}. n_features = {n_features}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"]},{"cell_type":"markdown","metadata":{"id":"us9M2NwOPhri"},"source":["### Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEfcN_CWPldl"},"outputs":[],"source":["from google.colab import files\n","import datetime\n","import json\n","\n","\n","json_string = json.dumps(results_log, indent=4)\n","\n","# Store the results as a JSON\n","filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Consolidation Results {d.short_name}.json\"\n","with open(filename, \"w\") as f:\n","    f.write(json_string)\n","\n","# Download the results\n","files.download(filename)\n","\n","\n","# Convert the results log to a Pandas DataFrame\n","df_results = pd.DataFrame(results_log)\n","\n","# Save the results as a CSV\n","filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Consolidation Results {d.short_name}.csv\"\n","df_results.to_csv(filename, index=False)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvOZtzqKRINh"},"outputs":[],"source":["from google.colab import files\n","import json\n","\n","# Upload the results JSON\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Load the scores\n","results_log = json.loads(uploaded[filename])\n","len(results_log)"]},{"cell_type":"markdown","metadata":{"id":"eTPGzuqyoSj5"},"source":["### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crXgaQzFycXJ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Convert the results log to a Pandas DataFrame\n","df_results = pd.DataFrame(results_log)\n","\n","# Calcualte average F1 scores from Random Forest and Logistic Regression\n","df_results[\"F1 Score\"] = df_results.apply(lambda row: np.mean(row[[\"f1_rf\", \"f1_lr\"]]), axis=1)\n","\n","df_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AH9DrtJUyfha"},"outputs":[],"source":["df_results.groupby([\"dataset\", \"method\"]).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PZaOjbGHx-8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Convert the results log to a Pandas DataFrame\n","df_results = pd.DataFrame(results_log)\n","\n","# Calcualte average F1 scores from Random Forest and Logistic Regression\n","df_results[\"F1 Score\"] = df_results.apply(lambda row: np.mean(row[[\"f1_rf\", \"f1_lr\"]]), axis=1)\n","\n","df_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJl6awx3UteY"},"outputs":[],"source":["df_results.groupby([\"dataset\", \"method\"]).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-kB_R10mXnZ"},"outputs":[],"source":["# Simplify the dataset IDs for visualization\n","df_results[\"dataset\"] = df_results[\"dataset\"].map({\n","    \"cardiffnlp/tweet_sentiment_multilingual\": \"Sentiment\",\n","    \"hate_speech18\": \"Hate Speech\",\n","    \"amazon_polarity\": \"Amazon\",\n","    \"GonzaloA/fake_news\": \"Fake News\",\n","    \"tum-nlp/IDMGSP\": \"Papers\"\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFpb8BRDtjhS"},"outputs":[],"source":["# Set the order for the datasets\n","datasets_order = [\n","    \"Sentiment\",\n","    \"Hate Speech\",\n","    \"Amazon\",\n","    \"Fake News\",\n","    \"Papers\"\n","]\n","\n","# Set the order for the legend\n","methods_order = [\n","    \"Random Feature Selection\",\n","    \"Supervised Forward Selection\",\n","    \"Agglomerative Clustering\",\n","    \"K-means\",\n","    \"HDBSCAN\"\n","]\n","\n","# Define a color for each method\n","colors = {\n","    \"Supervised Forward Selection\": \"#969696\",\n","    \"HDBSCAN\": \"#e6550d\",\n","    \"Agglomerative Clustering\": \"#6baed6\",\n","    \"K-means\": \"#74c476\",\n","    \"Random Feature Selection\": \"#d9d9d9\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pxuqd9-IY0D9"},"outputs":[],"source":["import matplotlib.ticker as mtick\n","\n","# Group by 'n_features' and 'method' to calculate the mean of 'F1 Score' for duplicated entries (e.g., for random feature selection which has been executed multiple times)\n","df_adjusted = df_results.groupby(['n_features', 'method', 'dataset'])['F1 Score'].mean().reset_index()\n","\n","# Drop entries for HDBSCAN with noise\n","df_adjusted = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN (w/ noise)\"]\n","df_adjusted[\"method\"] = df_adjusted[\"method\"].replace(\"HDBSCAN (w/o noise)\", \"HDBSCAN\")\n","df_adjusted = df_adjusted.reset_index()\n","\n","# Create a grid of 2x3 for the scatter plots\n","fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n","axes = axes.flatten()\n","\n","# Plot a scatter plot for each dataset\n","for i, dataset in enumerate(datasets_order):\n","    df_subset = df_adjusted[df_adjusted['dataset'] == dataset]\n","    for method in methods_order:\n","        df_method = df_subset[df_subset['method'] == method]\n","        axes[i].scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=(15 if method == \"HDBSCAN\" else 3), color=colors[method], zorder=3)\n","    axes[i].set_title(dataset)\n","    axes[i].set_ylabel('F1 Score' if i % 2 == 0 else '')\n","    axes[i].set_xlabel('Number of Selected Features' if i > 3 else '')\n","    axes[i].set_xlim([0, max(df_subset['n_features'])])\n","    axes[i].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n","    axes[i].grid(True, color='lightgrey', zorder=0)\n","\n","# Plot a scotter plot of the average across all datasets\n","df_global_average = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN\"].groupby(['n_features', 'method']).mean(numeric_only=True).reset_index()\n","hdbscan_average = df_adjusted[df_adjusted[\"method\"] == \"HDBSCAN\"][[\"n_features\", \"F1 Score\"]].mean()\n","least_features = df_adjusted[[\"dataset\", \"n_features\"]].groupby(\"dataset\").max()[\"n_features\"].min()\n","for method in methods_order:\n","    df_method = df_global_average[df_global_average['method'] == method]\n","    axes[5].scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=3, color=colors[method], zorder=3)\n","axes[5].scatter(hdbscan_average[\"n_features\"], hdbscan_average[\"F1 Score\"], label=\"HDBSCAN\", s=15, color=colors[\"HDBSCAN\"], zorder=3)\n","axes[5].set_title(\"Average across Datasets\")\n","axes[5].set_xlim([0, least_features])\n","axes[5].set_xlabel('Number of Selected Features')\n","axes[5].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n","axes[5].grid(True, color='lightgrey', zorder=0)\n","\n","# Create a single legend for all plots\n","handles, labels = axes[0].get_legend_handles_labels()\n","fig.legend(handles, labels, loc='upper center', ncol=len(methods_order), bbox_to_anchor=(0.5, 1.02))\n","\n","# Add a chart title\n","fig.suptitle('Feature Consolidation Performance (Categorical Features)', y=1.05)\n","\n","# Adjust the layout\n","plt.tight_layout()\n","\n","# Download the plot as PDF\n","filename = \"Feature Consolidation Methods Categorical (All Datasets).pdf\"\n","plt.savefig(filename, bbox_inches='tight')\n","files.download(filename)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oj6cUsoqHtN6"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","# Group by 'n_features' and 'method' to calculate the mean of 'F1 Score' for duplicated entries (e.g., for random feature selection which has been executed multiple times)\n","df_adjusted = df_results.groupby(['n_features', 'method', 'dataset'])['F1 Score'].mean().reset_index()\n","\n","# Drop entries for HDBSCAN with noise\n","df_adjusted = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN (w/ noise)\"]\n","df_adjusted[\"method\"] = df_adjusted[\"method\"].replace(\"HDBSCAN (w/o noise)\", \"HDBSCAN\")\n","df_adjusted = df_adjusted.reset_index()\n","\n","# Calculate average results across all datasets\n","df_global_average = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN\"].groupby(['n_features', 'method']).mean(numeric_only=True).reset_index()\n","least_features = df_adjusted[[\"dataset\", \"n_features\"]].groupby(\"dataset\").max()[\"n_features\"].min()\n","\n","# Create a plot\n","fig = plt.figure(figsize=(6, 3.5))\n","\n","# Add results for each consolidation method\n","for method in methods_order:\n","    if method == \"HDBSCAN\":\n","        hdbscan_average = df_adjusted[df_adjusted[\"method\"] == \"HDBSCAN\"][[\"n_features\", \"F1 Score\"]].mean()\n","        plt.scatter(hdbscan_average[\"n_features\"], hdbscan_average[\"F1 Score\"], label=\"HDBSCAN\", s=25, color=colors[\"HDBSCAN\"], zorder=3)\n","    else:\n","        df_method = df_global_average[df_global_average['method'] == method]\n","        plt.scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=3, color=colors[method], zorder=3)\n","\n","# Format the plot\n","plt.title(\"Feature Consolidation Performance (Categorical Features)\")\n","plt.xlim([0, least_features + 0.5])\n","plt.ylim([0.57, 0.87])\n","plt.xlabel('Number of Selected Features')\n","plt.ylabel('F1 Score')\n","plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n","plt.grid(True, color='lightgrey', zorder=0)\n","plt.legend()\n","plt.tight_layout()\n","\n","# Download the plot as PDF\n","filename = \"Feature Consolidation Methods Categorical (Average).pdf\"\n","plt.savefig(filename, bbox_inches='tight')\n","files.download(filename)\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlaI6pvAURpf"},"outputs":[],"source":["from google.colab import files\n","\n","df_feature_selection = pd.DataFrame({\"Forward Selection\": scores_forward, \"K-means\": scores_kmeans, \"Agglomerative\": scores_agg, \"Random\": scores_random})\n","\n","filename = \"2023_11_02 - FELIX Feature Selection F1 Scores Fake News.csv\"\n","df_feature_selection.to_csv(filename, index=False)\n","\n","# Download the results\n","files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhStKTtc8qwH"},"outputs":[],"source":["from google.colab import files\n","\n","# Upload the results\n","uploaded = files.upload()\n","filename = list(uploaded.keys())[0]\n","\n","# Load the scores\n","df_feature_selection = pd.read_csv(filename)\n","\n","scores_forward = df_feature_selection[\"Forward Selection\"].tolist()\n","scores_kmeans = df_feature_selection[\"K-means\"].tolist()\n","scores_agg = df_feature_selection[\"Agglomerative\"].tolist()\n","scores_random = df_feature_selection[\"Random\"].tolist()"]},{"cell_type":"markdown","metadata":{"id":"z_fPiP34H0zV"},"source":["## Experiment 5.2: Scoring Validity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ln-30kIZE3N"},"outputs":[],"source":["from collections import Counter\n","\n","def gini_index(categories):\n","    # Count the occurrences of each category\n","    category_counts = Counter(categories)\n","    # Calculate the proportion of each category and square the proportions\n","    squared_proportions = [(count / len(categories)) ** 2 for count in category_counts.values()]\n","    # Sum the squared proportions and subtract from 1\n","    gini = 1 - sum(squared_proportions)\n","    return gini\n","\n","# Given array\n","categories = [\"A\", \"A\", \"C\", \"A\", \"B\", \"C\"]\n","categories = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n","\n","# Calculate Gini index\n","gini = gini_index(categories)\n","\n","print(f\"Gini index of the array: {gini}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xD4qeV3AgmGR"},"source":["### Setup: Learn Numeric Feature Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkgdBkrBOrRV"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxA2lp3jeR6T"},"outputs":[],"source":["# Select a dataset\n","d = dataset_list[\"papers\"]\n","d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rC_acG8Bd6Hc"},"outputs":[],"source":["# Fit FELIX to the dataset to learn features\n","felix = FELIX(\n","    context=d.context,\n","    temperature_scoring=0.0,\n","    discrete_features=False,\n","    verbose=True\n",")\n","\n","felix.fit(d.X_train, d.y_train)\n","\n","# Store the features learned by FELIX\n","feature_set = felix._features"]},{"cell_type":"markdown","metadata":{"id":"fHUEert8a5x0"},"source":["### Case: Stable Feature Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqtZshJUbEGr"},"outputs":[],"source":["# Get a random example\n","sample_row = d.X_test.sample(1, random_state=seed)\n","\n","# Replicate the example 10 times and create a new dataframe\n","df = pd.concat([sample_row]*10, ignore_index=True)\n","\n","# Score the example using FELIX\n","print(\"Scoring the same example 10 times ...\")\n","df_scores_stable = felix.transform(df)\n","\n","# Score different examples using FELIX\n","print(\"Scoring 10 different examples ...\")\n","df_sample = d.X_test.sample(10, random_state=seed)\n","df_scores_general = felix.transform(df_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkXuwhCElPOk"},"outputs":[],"source":["df_variance = pd.DataFrame({\"Inner-sample\": df_scores_stable.var(), \"Inter-sample\": df_scores_general.var()})\n","\n","plt.figure(figsize=(8, 2))\n","ax = df_variance.boxplot(vert=False)\n","ax.set_xlabel(\"Score variance\")\n","ax.set_title(\"Distribution of score variance inner-sample vs. inter-sample\")\n","plt.tight_layout()\n","plt.show()\n","\n","df_variance.describe()"]},{"cell_type":"markdown","metadata":{"id":"h3rwq4_wmk9M"},"source":["Alternative plot showing the score distribution per feature:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rxi6mHIRhvgf"},"outputs":[],"source":["# Create a boxplot for the value distribution of each feature\n","plt.figure(figsize=(8, df_scores_stable.shape[1] / 5))\n","ax = df_scores_stable.boxplot(vert=False)\n","ax.set_xlabel('Value')\n","ax.set_title('Score distribution (stable order)')\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FkfJ6_FZacN6"},"source":["### Case: Random Reordering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_C4ztCrKafRL"},"outputs":[],"source":["# Get a random example\n","sample_row = d.X_test.sample(1, random_state=seed)\n","\n","# Keep the scoring LLM stable with a 16K context window\n","felix.llm_scoring = \"gpt-3.5-turbo-16k\"\n","felix.verbose = True\n","\n","# Score the original feature set 10 times\n","print(\"Scoring the original feature set ...\")\n","felix._features = feature_set\n","results_rows_original = []\n","for _ in tqdm(range(10)):\n","    results_rows_original.append(felix.transform(sample_row))\n","df_scores_original = pd.concat(results_rows_original).reset_index(drop=True)\n","\n","# Score a randomly reordered feature set 10 times\n","print(\"Scoring the randomly reordered feature set ...\")\n","random.shuffle(felix._features.features) # Randomly reorders the feature set in place\n","results_rows_random = []\n","for _ in tqdm(range(10)):\n","    results_rows_random.append(felix.transform(sample_row))\n","df_scores_random = pd.concat(results_rows_random).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJctgpp0gAHJ"},"outputs":[],"source":["# Perform t-test to see if changing the scoring LLM results in significantly different scores\n","def test_significance(df_model_a, df_model_b):\n","    p_values = {}\n","    for column in df_model_a.columns:\n","        _, p_value = stats.ttest_rel(df_model_a[column], df_model_b[column])\n","        p_values[column] = p_value\n","    return pd.Series(p_values)\n","\n","p_values = test_significance(df_scores_original, df_scores_random)\n","\n","# Excluding NaN values for plotting\n","p_values_cleaned = p_values.dropna()\n","\n","# Plotting boxplots\n","plt.figure(figsize=(7, 1.5))\n","plt.boxplot([p_values_cleaned], vert=False, labels=[\"Random reordering\"])\n","plt.xlabel(\"p-value\")\n","plt.title(\"t-test for score differences when randomly reordering the feature set\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Computing statistics\n","stats_random_reordering = {\n","    \"Avg. p-value\": [p_values_cleaned.mean()],\n","    \"% of p-values < 0.01\": [(p_values_cleaned < 0.01).mean() * 100],\n","    \"% of p-values < 0.05\": [(p_values_cleaned < 0.05).mean() * 100],\n","    \"% of p-values < 0.1\": [(p_values_cleaned < 0.1).mean() * 100]\n","}\n","\n","# Creating a DataFrame\n","df_stats_random_reordering = pd.DataFrame(stats_random_reordering, index=[\"Random reordering\"])\n","df_stats_random_reordering"]},{"cell_type":"markdown","metadata":{"id":"hHblwbnRfyHi"},"source":["Alternative experiment showing the large variance in scores when the feature set is randomly reschuffled in each scoring request:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOdWDH23juEC"},"outputs":[],"source":["# Get a random example\n","sample_row = d.X_test.sample(1, random_state=seed)\n","\n","# List to store the transformed rows\n","result_rows = []\n","\n","for _ in range(10):\n","    # Randomly reorder the features\n","    random.shuffle(felix._features.features)\n","    print(f\"Iteration {_}: first feature is {felix._features.features[0].name}\")\n","\n","    # Transform the sample row and append the result to the list\n","    transformed_row = felix.transform(sample_row)\n","    result_rows.append(transformed_row)\n","\n","# Concatenate all resulting rows into a single dataframe\n","df_scores_random = pd.concat(result_rows).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWHIfaumneGi"},"outputs":[],"source":["# Create a boxplot for the value distribution of each feature\n","plt.figure(figsize=(8, df_scores_random.shape[1] / 5))\n","ax = df_scores_random.boxplot(vert=False)\n","ax.set_xlabel('Value')\n","ax.set_title('Score distribution (random order)')\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BiD9UCwjGe28"},"source":["### Case: Splitting scoring requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vf9YlmIME8oq"},"outputs":[],"source":["# Get a random example\n","sample_row = d.X_test.sample(1, random_state=seed)\n","\n","# Set scoring LLM to the 16K context variant of GPT-3.5 so that all features fit into the context of one request\n","felix.llm_scoring=\"gpt-3.5-turbo-16k\"\n","felix.verbose=False\n","\n","# Score the example 10 times with all features in one request\n","print(\"Scoring with all features in one request ...\")\n","felix._features = feature_set\n","result_rows_no_split = []\n","for _ in tqdm(range(10)):\n","    # Transform the sample row and append the result to the list\n","    result_rows_no_split.append(felix.transform(sample_row))\n","df_scores_no_split = pd.concat(result_rows_no_split).reset_index(drop=True)\n","\n","# Score the example 10 times with only the first half of the feature set\n","print(\"Scoring wih first half of the feature set ...\")\n","felix._features = NumericalFeatureSet(features=feature_set.features[:len(feature_set.features)//2])\n","result_rows_split_a = []\n","for _ in tqdm(range(10)):\n","    # Transform the sample row and append the result to the list\n","    result_rows_split_a.append(felix.transform(sample_row))\n","df_scores_split_a = pd.concat(result_rows_split_a).reset_index(drop=True)\n","\n","# Score the example 10 times with only the second half of the feature set\n","print(\"Scoring wih second half of the feature set ...\")\n","felix._features = NumericalFeatureSet(features=feature_set.features[len(feature_set.features)//2:])\n","result_rows_split_b = []\n","for _ in tqdm(range(10)):\n","    # Transform the sample row and append the result to the list\n","    result_rows_split_b.append(felix.transform(sample_row))\n","df_scores_split_b = pd.concat(result_rows_split_b).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3KEn8qsSI8g"},"outputs":[],"source":["# Perform t-test to see if splitting up scoring into multiple requests results in significantly different scores\n","def test_significance(df_split, df_no_split):\n","    p_values = {}\n","    for column in df_split.columns:\n","        if column in df_no_split.columns:\n","            _, p_value = stats.ttest_rel(df_split[column], df_no_split[column])\n","            p_values[column] = p_value\n","    return pd.Series(p_values)\n","\n","p_values_split_a = test_significance(df_scores_split_a, df_scores_no_split)\n","p_values_split_b = test_significance(df_scores_split_b, df_scores_no_split)\n","\n","# Excluding NaN values for plotting\n","p_values_a_cleaned = p_values_split_a.dropna()\n","p_values_b_cleaned = p_values_split_b.dropna()\n","\n","# Plotting boxplots\n","plt.figure(figsize=(7, 3))\n","plt.boxplot([p_values_a_cleaned, p_values_b_cleaned], vert=False, labels=[\"First half\", \"Second half\"])\n","plt.xlabel(\"p-value\")\n","plt.title(\"t-test for score differences in first and second half of feature set vs. full feature set\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Computing statistics\n","stats_split = {\n","    \"Avg. p-value\": [p_values_a_cleaned.mean(), p_values_b_cleaned.mean()],\n","    \"% of p-values < 0.01\": [(p_values_a_cleaned < 0.01).mean() * 100, (p_values_b_cleaned < 0.01).mean() * 100],\n","    \"% of p-values < 0.05\": [(p_values_a_cleaned < 0.05).mean() * 100, (p_values_b_cleaned < 0.05).mean() * 100],\n","    \"% of p-values < 0.1\": [(p_values_a_cleaned < 0.1).mean() * 100, (p_values_b_cleaned < 0.1).mean() * 100]\n","}\n","\n","# Creating a DataFrame\n","df_stats_split = pd.DataFrame(stats_split, index=[\"First half\", \"Second half\"])\n","df_stats_split"]},{"cell_type":"markdown","metadata":{"id":"NmDaAXcbT0yg"},"source":["### Case: Changing LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-IR848HNnKX"},"outputs":[],"source":["# Get a random example\n","sample_row = d.X_test.sample(1, random_state=seed)\n","\n","# Limit the number of features to score due to small context window\n","max_features = 40\n","felix._features = NumericalFeatureSet(features=feature_set.features[:min(max_features, len(feature_set.features))])\n","felix.verbose=True\n","\n","# Score the example 10 times with 4K context window\n","print(\"Scoring with with 4K context window ...\")\n","felix.llm_scoring=\"gpt-3.5-turbo\"\n","result_rows_4k = []\n","for _ in tqdm(range(10)):\n","    # Transform the sample row and append the result to the list\n","    result_rows_4k.append(felix.transform(sample_row))\n","df_scores_4k = pd.concat(result_rows_4k).reset_index(drop=True)\n","\n","# Score the example 10 times with 16K context window\n","print(\"Scoring with with 16K context window ...\")\n","felix.llm_scoring=\"gpt-3.5-turbo-16k\"\n","result_rows_16k = []\n","for _ in tqdm(range(10)):\n","    # Transform the sample row and append the result to the list\n","    result_rows_16k.append(felix.transform(sample_row))\n","df_scores_16k = pd.concat(result_rows_16k).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOrMIdiwVQsJ"},"outputs":[],"source":["# Perform t-test to see if changing the scoring LLM results in significantly different scores\n","def test_significance(df_model_a, df_model_b):\n","    p_values = {}\n","    for column in df_model_a.columns:\n","        _, p_value = stats.ttest_rel(df_model_a[column], df_model_b[column])\n","        p_values[column] = p_value\n","    return pd.Series(p_values)\n","\n","p_values = test_significance(df_scores_4k, df_scores_16k)\n","\n","# Excluding NaN values for plotting\n","p_values_cleaned = p_values.dropna()\n","\n","# Plotting boxplots\n","plt.figure(figsize=(7, 1.5))\n","plt.boxplot([p_values_cleaned], vert=False, labels=[\"Model change\"])\n","plt.xlabel(\"p-value\")\n","plt.title(\"t-test for score differences when changing the scoring LLM\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Computing statistics\n","stats_model_change = {\n","    \"Avg. p-value\": [p_values_cleaned.mean()],\n","    \"% of p-values < 0.01\": [(p_values_cleaned < 0.01).mean() * 100],\n","    \"% of p-values < 0.05\": [(p_values_cleaned < 0.05).mean() * 100],\n","    \"% of p-values < 0.1\": [(p_values_cleaned < 0.1).mean() * 100]\n","}\n","\n","# Creating a DataFrame\n","df_stats_model_change = pd.DataFrame(stats_model_change, index=[\"Model change\"])\n","df_stats_model_change"]},{"cell_type":"markdown","metadata":{"id":"gcO0HGZofFsn"},"source":["### E2E Performance when Reschuffling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8u5KQ7mnfiBv"},"outputs":[],"source":["d = dataset_list[\"papers\"]\n","d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YQrsqEQfJsE"},"outputs":[],"source":["callback = CustomFELIXCallback()\n","felix = FELIX(context=d.context_train, discrete_features=False, callback=callback, verbose=True)\n","\n","felix.fit(d.X_train, d.y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wf7kExMgeBU"},"outputs":[],"source":["original_features = felix._features\n","len(original_features.features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBq07sqtfb3Q"},"outputs":[],"source":["felix.reschuffle_features = True\n","\n","# Transform data using feature reschuffling\n","df_scores_train_reschuffle = felix.transform(d.X_train)\n","df_scores_test_reschuffle = felix.transform(d.X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTMNrbKk6JXK"},"outputs":[],"source":["data_transformer = DataTransformer(dataset=d)\n","df_scores_train_reschuffle_imputed, df_scores_test_reschuffle_imputed = data_transformer.impute_missing_values(df_scores_train_reschuffle, df_scores_test_reschuffle)\n","\n","columns = [col for col in df_scores_train_reschuffle_imputed.columns if col in df_scores_test_reschuffle_imputed.columns]\n","\n","df_scores_train_reschuffle_imputed = df_scores_train_reschuffle_imputed[columns]\n","df_scores_test_reschuffle_imputed = df_scores_test_reschuffle_imputed[columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4icf74q5oha"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","\n","rf = RandomForestClassifier()\n","rf.fit(df_scores_train_reschuffle_imputed, d.y_train)\n","y_pred_rf = rf.predict(df_scores_test_reschuffle_imputed)\n","\n","print(\"F1 Random Forest:\", f1_score(d.y_test, y_pred_rf, average=\"macro\"))\n","\n","lr = LogisticRegression(max_iter=1000)\n","lr.fit(df_scores_train_reschuffle_imputed, d.y_train)\n","y_pred_lr = lr.predict(df_scores_test_reschuffle_imputed)\n","\n","print(\"F1 Logistic Regression:\", f1_score(d.y_test, y_pred_lr, average=\"macro\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KW1ajn5Jh8Po"},"outputs":[],"source":["felix.reschuffle_features = False\n","felix._features = original_features\n","\n","df_scores_train_stable = felix.transform(d.X_train)\n","df_scores_test_stable = felix.transform(d.X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCzH1yN8Dfrm"},"outputs":[],"source":["data_transformer = DataTransformer(dataset=d)\n","df_scores_train_stable_imputed, df_scores_test_stable_imputed = data_transformer.impute_missing_values(df_scores_train_stable, df_scores_test_stable)\n","\n","columns = [col for col in df_scores_train_stable_imputed.columns if col in df_scores_test_stable_imputed.columns]\n","\n","df_scores_train_stable_imputed = df_scores_train_stable_imputed[columns]\n","df_scores_test_stable_imputed = df_scores_test_stable_imputed[columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1X8Iu-AYiGLp"},"outputs":[],"source":["rf = RandomForestClassifier()\n","rf.fit(df_scores_train_stable_imputed, d.y_train)\n","y_pred_rf = rf.predict(df_scores_test_stable_imputed)\n","\n","print(\"F1 Random Forest:\", f1_score(d.y_test, y_pred_rf, average=\"macro\"))\n","\n","lr = LogisticRegression(max_iter=1000)\n","lr.fit(df_scores_train_stable_imputed, d.y_train)\n","y_pred_lr = lr.predict(df_scores_test_stable_imputed)\n","\n","print(\"F1 Logistic Regression:\", f1_score(d.y_test, y_pred_lr, average=\"macro\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzSVy4wViMQZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyOB+Hsq40fqb8uNwMTQLrrp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}